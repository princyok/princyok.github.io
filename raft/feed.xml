<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-02T20:49:45-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Prince’s scribbles</title><subtitle>My repository of some things that I've been thinking about.</subtitle><author><name>Prince Okoli</name></author><entry><title type="html">Catching AI with its pants down: Implement an Artificial Neuron from Scratch.</title><link href="http://localhost:4000/implement-an-artificial-neuron-from-scratch.html" rel="alternate" type="text/html" title="Catching AI with its pants down: Implement an Artificial Neuron from Scratch." /><published>2020-04-04T00:00:00-06:00</published><updated>2020-04-04T00:00:00-06:00</updated><id>http://localhost:4000/implement-an-artificial-neuron-from-scratch</id><content type="html" xml:base="http://localhost:4000/implement-an-artificial-neuron-from-scratch.html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
          }
        });
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;
&lt;/script&gt;

&lt;table&gt;
&lt;td&gt;
&lt;i&gt;We will strip the mighty, massively hyped, highly dignified AI of its cloths, and bring its innermost details down to earth!&lt;/i&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#prologue&quot; id=&quot;markdown-toc-prologue&quot;&gt;&lt;strong&gt;Prologue&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#code-implementation-an-artificial-neuron&quot; id=&quot;markdown-toc-code-implementation-an-artificial-neuron&quot;&gt;&lt;strong&gt;Code Implementation: an artificial neuron&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#constructor&quot; id=&quot;markdown-toc-constructor&quot;&gt;&lt;strong&gt;Constructor&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#parameter-initialization&quot; id=&quot;markdown-toc-parameter-initialization&quot;&gt;&lt;strong&gt;Parameter initialization&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#forward-pass&quot; id=&quot;markdown-toc-forward-pass&quot;&gt;&lt;strong&gt;Forward pass&lt;/strong&gt;&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#activation-function&quot; id=&quot;markdown-toc-activation-function&quot;&gt;&lt;strong&gt;Activation function&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calculation-of-cost&quot; id=&quot;markdown-toc-calculation-of-cost&quot;&gt;&lt;strong&gt;Calculation of Cost&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backward-pass&quot; id=&quot;markdown-toc-backward-pass&quot;&gt;&lt;strong&gt;Backward pass&lt;/strong&gt;&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#update-parameters-via-gradient-descent&quot; id=&quot;markdown-toc-update-parameters-via-gradient-descent&quot;&gt;&lt;strong&gt;Update parameters via gradient descent&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#training&quot; id=&quot;markdown-toc-training&quot;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#evaluation-of-trained-artificial-neuron&quot; id=&quot;markdown-toc-evaluation-of-trained-artificial-neuron&quot;&gt;&lt;strong&gt;Evaluation of trained artificial neuron&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#all-the-codes&quot; id=&quot;markdown-toc-all-the-codes&quot;&gt;&lt;strong&gt;All the codes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prologue&quot;&gt;&lt;strong&gt;Prologue&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;This is part 4 of the blog series, &lt;em&gt;Catching AI with its pants down&lt;/em&gt;. In this part we will implement all the equation that we derived from scratch in the previous parts.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parts&lt;/th&gt;
      &lt;th&gt;Catching AI with its pants down&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/some-musings-about-ai.html&quot;&gt;&lt;strong&gt;Some Musings About AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 2&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/understand-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Understand an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 3&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/optimize-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Optimize an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 4&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/implement-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Implement an artificial neuron from scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 5&lt;/td&gt;
      &lt;td&gt;Understand a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 6&lt;/td&gt;
      &lt;td&gt;Optimize a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 7&lt;/td&gt;
      &lt;td&gt;Implement a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;code-implementation-an-artificial-neuron&quot;&gt;&lt;strong&gt;Code Implementation: an artificial neuron&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;All the codes will be in Python, using its object-oriented paradigm wherever possible (but I won’t bother with &lt;a href=&quot;https://en.wikipedia.org/wiki/Mutator_method&quot; target=&quot;_blank&quot;&gt;getters and setters&lt;/a&gt; for the most part). We will use primarily the &lt;a href=&quot;https://numpy.org/&quot; target=&quot;_blank&quot;&gt;NumPy library&lt;/a&gt; because its operations are very efficient for linear algebra computations involving arrays.&lt;/p&gt;

&lt;p&gt;This implementation does not take advantage of parallel computing, so your GPU won’t make things any faster. But it takes advantage of NumPy’s superb optimization for computations with multidimensional arrays. Therefore, python loops are avoided as much as possible in the code, which is why we went through all that work to have everything as tensors.&lt;/p&gt;

&lt;p&gt;We will also not implement any concurrent computing (so no multithreading of any sort) other than any that may have been baked into NumPy. Most deep learning libraries include concurrent and parallel computing capabilities, and also automatic differentiation capability. Moreover, none of those are really needed for a single artificial neuron. But they are absolutely priceless when training a network of neurons (a.k.a. neural network).&lt;/p&gt;

&lt;h4 id=&quot;constructor&quot;&gt;&lt;strong&gt;Constructor&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;We begin by implementing our constructor, where we initialize all our data members (also using it as an opportunity to lay them all out).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We don’t really need to access the entire data (&lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt;) during its instantiation. We could have chosen to initialize &lt;code class=&quot;highlighter-rouge&quot;&gt;self.X&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;self.Y&lt;/code&gt; later. We only just needed the shape of &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;, because we use it to get the number of features in our data which we use when we initialize our parameters. However, I chose to have both &lt;code class=&quot;highlighter-rouge&quot;&gt;self.X&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;self.Y&lt;/code&gt; initialized at instantiation for the sake of it, so this is certainly an opportunity for some refactoring to improve the code.&lt;/p&gt;

&lt;h3 id=&quot;parameter-initialization&quot;&gt;&lt;strong&gt;Parameter initialization&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We first initialize our parameters, and we will do this randomly.&lt;/p&gt;

&lt;p&gt;Next, we will implement a method for parameter initialization. It’s just going to be plain random initialization.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;forward-pass&quot;&gt;&lt;strong&gt;Forward pass&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Forward pass can be broken into two steps: First is the linear combination of the parameters and datapoint values to get the preactivation. Next is the passing of the preactivation through an activation function to get the activation.&lt;/p&gt;

&lt;p&gt;The equations for forward pass are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{z}=\vec{w}\mathbf{X}+b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{a}=f\left(\vec{z}\right)&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_logistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that that I used &lt;code class=&quot;highlighter-rouge&quot;&gt;self.X_batch&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;self.X&lt;/code&gt;, because we perform our calculations on batches of samples from the dataset. We will initialize &lt;code class=&quot;highlighter-rouge&quot;&gt;self.X_batch&lt;/code&gt; during training (i.e. inside the &lt;code class=&quot;highlighter-rouge&quot;&gt;train&lt;/code&gt; method).&lt;/p&gt;

&lt;h4 id=&quot;activation-function&quot;&gt;&lt;strong&gt;Activation function&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Next, we implement out activation function. We will only do logistic for this model of an artificial neuron. Check out the deep neural network code for some other activation functions.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{a}=f\left(\vec{z}\right)=\frac{1}{1+e^{-\vec{z}}}&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_logistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We will also implement the derivate of the activation function (we are using the logistic function). But note that we invoke this method only during backward pass, not forward pass. Presenting it here (and writing the code near that for the forward pass) is just a matter of personal taste.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f'\left(\vec{z}\right)=\vec{a}\odot\left(1-\vec{a}\right)&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_logistic_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;calculation-of-cost&quot;&gt;&lt;strong&gt;Calculation of Cost&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Next, we should implement the method for computing the cost, but I didn’t do it for artificial neuron, but instead did it for the main thing, the deep neural network code, and the blog post for it is coming soon.&lt;/p&gt;

&lt;p&gt;Note that you don’t actually need the cost for the training process, but instead the cost gradients. The cost is just there to tell us how the training is progressing.
This is the equation we would implement.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J=-\frac{1}{m}\bullet\sum_{j}^{m}{y_i\cdot \log{(y}_i)+(1-a_i)\bullet\log({1-a}_i)}&lt;/script&gt;

&lt;h3 id=&quot;backward-pass&quot;&gt;&lt;strong&gt;Backward pass&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Now we will optimize our parameters in such a way that our loss decreases. We start by first computing the cost gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{w}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{w}}=\frac{\partial J}{\partial\vec{z}}\frac{\partial\vec{z}}{\partial\vec{w}}=\ \frac{\partial J}{\partial\vec{z}}X^T=\frac{\partial J}{\partial\vec{a}}\odot\frac{\partial\vec{a}}{\partial\vec{z}}X^T=\frac{\partial J}{\partial\vec{a}}\odot f'(\vec{z})X^T&lt;/script&gt;

&lt;p&gt;For a logistic loss function and a logistic activation function, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{w}}=-\frac{1}{m}\bullet\left(\frac{\vec{y}}{\vec{a}}-\frac{1-\vec{y}}{1-\vec{a}}\right)\ \odot(\vec{a}\odot\left(1-\vec{a}\right))X^T&lt;/script&gt;

&lt;p&gt;We could directly implement the above equation, but I chose to implement it in stages, with each gradient computed at each stage. This will make it a little easier to swap in other activation functions and loss functions in the future (I don’t really have any intention to do so for the artificial neuron code, as I already did it in the deep neural network code).&lt;/p&gt;

&lt;p&gt;So, we implement the following equations step by step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\vec{a}}{\partial\vec{z}}:=f'\left(\vec{z}\right)=\vec{a}\odot\left(1-\vec{a}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{a}}=-\frac{1}{m}\bullet\left(\frac{\vec{y}}{\vec{a}}-\frac{1-\vec{y}}{1-\vec{a}}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{z}}=\frac{\partial J}{\partial\vec{a}}\odot\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{w}}=\ \frac{\partial J}{\partial\vec{z}}X^T&lt;/script&gt;

&lt;p&gt;The cost gradients for the bias is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\sum_{j=1}^{m}\left(\frac{\partial J}{\partial\vec{z}}\right)_j&lt;/script&gt;

&lt;p&gt;As we showed in part 3, we can also choose to use this equation instead:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\frac{\partial J}{\partial\vec{z}}\ \frac{\partial\vec{z}}{\partial b}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \frac{\partial \vec{z}}{\partial b} $ is an $ m $-by-$ 1 $ vector of ones (i.e. has same shape as $ \vec{z}^T $).
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Both equations, implemented as &lt;code class=&quot;highlighter-rouge&quot;&gt;self.dJdB= np.sum(self.dJdZ, axis=1)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;self.dJdB= np.matmul(self.dJdZ, np.ones(self.z.T.shape))&lt;/code&gt;, produce the same result. We will use the former.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_logistic_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdA&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;update-parameters-via-gradient-descent&quot;&gt;&lt;strong&gt;Update parameters via gradient descent&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Next, we update &lt;em&gt;each&lt;/em&gt; parameter using gradient descent:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{new}=w_{old}-\gamma\frac{\partial J}{\partial w_{old}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_{new}=b_{old}-\gamma\frac{\partial J}{\partial b_{old}}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \gamma $ is the learning rate (a.k.a. step size). It's a hyperparameter, meaning that it is a variable you directly set and control.
&lt;br /&gt;&lt;br /&gt;
Note that $ \frac{\partial J}{\partial w_{old}} $ is simply the $ \frac{\partial J}{\partial w} $ that we just calculated, and the same is true for $ \frac{\partial J}{\partial b_{old}} $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;With this, we’ve completed one iteration of training. We repeat this as many times as we want. Eventually, we expect to end up with an artificial neuron that has learned the underlying relationship between the features and the target.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_parameters_via_gradient_descent&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdW&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdB&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;training&quot;&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The training process is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Randomly initialize our parameters&lt;/li&gt;
  &lt;li&gt;Run one iteration of training, which involves:
    &lt;ul&gt;
      &lt;li&gt;Sample a batch from our dataset.&lt;/li&gt;
      &lt;li&gt;Then run forward pass (i.e. move the data forward through the neuron).&lt;/li&gt;
      &lt;li&gt;Then run backward pass to calculate our cost gradients.&lt;/li&gt;
      &lt;li&gt;Then run gradient descent (which is technically part of backward pass), which uses the cost gradients to update the parameters.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Repeat step 2 until we reach the specified number of iterations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore we combine the code snippets accordingly:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training begins...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;random_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_parameters_via_gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training Complete!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have three hyperparameters we can use to tune the training process: number of iterations, learning rate, and batch size.&lt;/p&gt;

&lt;h3 id=&quot;evaluation-of-trained-artificial-neuron&quot;&gt;&lt;strong&gt;Evaluation of trained artificial neuron&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;And finally, we implement methods for evaluating the neuron, including method for computing accuracy and precision. These are very pretty straightforward.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_compute_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Caution: All the activations are null values.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_compute_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Caution: All the activations are null values.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;pred_positives_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_positives_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_positives_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;We&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bundle&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;methods&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;under&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluating&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;_available_perfomance_metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;precision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_available_perfomance_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_compute_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;precision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_compute_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I decided to get a little cheeky and throw &lt;code class=&quot;highlighter-rouge&quot;&gt;ValueError&lt;/code&gt; when an invalid string is passed to &lt;code class=&quot;highlighter-rouge&quot;&gt;metric&lt;/code&gt;, a formal parameter of the method evaluate.&lt;/p&gt;

&lt;p&gt;I also decided to print a warning message if all my activations are &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.13.0/user/misc.html&quot; target=&quot;_blank&quot;&gt;NaNs&lt;/a&gt; (i.e. null values). From my experience, these can occur when the computations cause an arithmetic overflow or underflow.&lt;/p&gt;

&lt;h2 id=&quot;all-the-codes&quot;&gt;&lt;strong&gt;All the codes&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;You can find the entire code, along with the code for deep neural network (the writeup for it is coming soon) and demonstrations using it to tackle real public research datasets, in &lt;a href=&quot;https://github.com/princyok/deep_learning_without_ml_libraries&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;this GitHub repo&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The version as of the end of March 2020 is repeated here for your convenience:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seterr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;over&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;warn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;under&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;warn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# warn for overflows and underflows.
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Neuron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_logistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_logistic_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_logistic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_logistic_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dAdZ&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdA&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdZ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_parameters_via_gradient_descent&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdW&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dJdB&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_compute_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Caution: All the activations are null values.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_compute_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Caution: All the activations are null values.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;pred_positives_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_positives_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_positives_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training begins...&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_iterations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;random_indices&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prng&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_indices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_parameters_via_gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Training Complete!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;_available_perfomance_metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;precision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_available_perfomance_metrics&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_compute_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;precision&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_compute_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See you in the next article!&lt;/p&gt;</content><author><name>Prince Okoli</name></author><summary type="html"></summary></entry><entry><title type="html">Catching AI with its pants down: Optimize an Artificial Neuron from Scratch</title><link href="http://localhost:4000/optimize-an-artificial-neuron-from-scratch.html" rel="alternate" type="text/html" title="Catching AI with its pants down: Optimize an Artificial Neuron from Scratch" /><published>2020-03-27T00:00:00-06:00</published><updated>2020-03-27T00:00:00-06:00</updated><id>http://localhost:4000/optimize-an-artificial-neuron-from-scratch</id><content type="html" xml:base="http://localhost:4000/optimize-an-artificial-neuron-from-scratch.html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
          }
        });
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;
&lt;/script&gt;

&lt;table&gt;
&lt;td&gt;
&lt;i&gt;We will strip the mighty, massively hyped, highly dignified AI of its cloths, and bring its innermost details down to earth!&lt;/i&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#prologue&quot; id=&quot;markdown-toc-prologue&quot;&gt;&lt;strong&gt;Prologue&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-descent-algorithm&quot; id=&quot;markdown-toc-gradient-descent-algorithm&quot;&gt;&lt;strong&gt;Gradient Descent Algorithm&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#chain-rule-for-cost-gradient&quot; id=&quot;markdown-toc-chain-rule-for-cost-gradient&quot;&gt;&lt;strong&gt;Chain rule for cost gradient&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prologue&quot;&gt;&lt;strong&gt;Prologue&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;This is part 3 of the blog series, &lt;em&gt;Catching AI with its pants down&lt;/em&gt;. In this part, I will dive into the mathematical details of training (optimizing) an artificial neuron via gradient descent. This will be a math-heavy article, so get your pen and scratch papers ready. But I made sure to simplify things.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parts&lt;/th&gt;
      &lt;th&gt;Catching AI with its pants down&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/some-musings-about-ai.html&quot;&gt;&lt;strong&gt;Some Musings About AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 2&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/understand-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Understand an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 3&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/optimize-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Optimize an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 4&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/implement-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Implement an artificial neuron from scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 5&lt;/td&gt;
      &lt;td&gt;Understand a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 6&lt;/td&gt;
      &lt;td&gt;Optimize a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 7&lt;/td&gt;
      &lt;td&gt;Implement a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let’s recap before we begin the last dash:&lt;/p&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Recall that an artificial neuron can be succinctly described as a function that takes in $ \mathbf{X} $ and uses its parameters $ \vec{w} $ to do some computations to spit out an activation value that we expect to be close to the actual correct value (the ground truth), $ \vec{y} $. This also means that we expect some level of error between the activation value and the ground truth, and the loss function gives us a measure of this error in the form of single scalar value, the loss (or cost).
&lt;br /&gt;&lt;br /&gt;
We want the activation to be as close as possible to the ground truth by getting the loss to be as small as possible. In order to do that, we want to find a set of values for $ \vec{w} $ such that the loss is always as low as possible.
&lt;br /&gt;&lt;br /&gt;
What remains to be seen is how we pull this off.
    &lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;gradient-descent-algorithm&quot;&gt;&lt;strong&gt;Gradient Descent Algorithm&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;As we &lt;a href=&quot;/understand-an-artificial-neuron-from-scratch.html#loss-function&quot; target=&quot;_blank&quot;&gt;saw in part 2&lt;/a&gt;, we have a loss function that is a function of the weights and biases, and we need a way to find the set of weights and biases that minimizes the loss. This is a clearcut optimization problem.&lt;/p&gt;

&lt;p&gt;There are many ways to solve this optimization problem, but we will go with the one that scales excellently with deep neural networks, since that is the eventual goal of this writeup. And that brings us to the gradient descent algorithm.&lt;/p&gt;

&lt;p&gt;We will illustrate how it works using a simple scenario where we have a dataset made of one feature and one target, and we want to use the mean square error as cost function. We specify a linear activation function (&lt;script type=&quot;math/tex&quot;&gt;a=f(a)&lt;/script&gt;) for the neuron. Then the equation for our neuron will be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a=f\left(z\right)=w_1\ \cdot x_1+w_0&lt;/script&gt;

&lt;p&gt;Our cost function becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J=\frac{1}{m}\cdot\sum_{j=0}^{m}{({y}_j-a_j)}^2=\frac{1}{m}\cdot\sum_{j=0}^{m}{(y_j-\ w_{1,j}\ \cdot x_{1,j}+w_{0,j})}^2&lt;/script&gt;

&lt;p&gt;Let’s further simplify our scenario by assuming we will only run computations for only one datapoint at a time.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J={(y_j-\ w_{1,j}\ \cdot x_{1,j}+w_{0,j})}^2&lt;/script&gt;

&lt;p&gt;If we hold &lt;script type=&quot;math/tex&quot;&gt;y_j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x_{1,j}&lt;/script&gt; constant, which is logical since they come directly from data, we observe that our cost is a function of just the parameters &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;. And we can easily plot the curve.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/error_vs_parameters.png&quot; alt=&quot;Plot of cost against two parameters.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Plot of cost against two parameters.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;From the plot we can easily see what values we can set &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; to in order to produce the most minimal cost. Any picks for &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; from the bottom of the valley will give a minimal cost.&lt;/p&gt;

&lt;p&gt;The gradient descent algorithm formalizes the idea we just followed. It pretty much says: Start somewhere on the cost function (in this case, the plotted surface) and only take steps in the direction of negative gradient (i.e. direction of descent). Once you hit a minimum, any step you take will always turn out to be in the direction of ascent and therefore the iteration will no longer improve the minimization of the cost.&lt;/p&gt;

&lt;p&gt;In mathematical terms, it is this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{new}=w_{old}-\gamma\frac{\partial J}{\partial w_{old}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_{new}=b_{old}-\gamma\frac{\partial J}{\partial b_{old}}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \gamma $ is the step size (a.k.a. learning rate).
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The above equations are used in updating the parameters at the end of each round or iteration of training. The equations are applied to each of the parameters in the model (the artificial neuron). For instance, for our toy dataset, there would be two weights, one for each feature (input variable) of the dataset, and one bias for the bias node. All three parameters will be updated using the equations. That marks the end of one round or iteration of training.&lt;/p&gt;

&lt;p&gt;Stochastic gradient descent means that randomization is introduced during the selection of the batch of datapoints to be used in the calculations of the gradient descent. Some people will distinguish further by defining mini-batch stochastic gradient descent as when a batch of datapoints is randomly selected from the dataset and used, while stochastic gradient descent refers to just using a single randomly selected datapoint for each entire round of computations.&lt;/p&gt;

&lt;p&gt;If we had more than two parameters, or a non-linear activation function, or some other property that makes our neuron more complicated, using a plot to find the parameters that minimize the error becomes just impractical. We must use the mathematical formulation.&lt;/p&gt;

&lt;p&gt;What remains to be answered is how we can efficiently compute &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial w_{old}}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b_{old}}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;chain-rule-for-cost-gradient&quot;&gt;&lt;strong&gt;Chain rule for cost gradient&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Heads up: For this part, which is the real meat of the training process, I advised that you bring out a pen and some paper to work along, especially if this is your first time working with Jacobians.&lt;/p&gt;

&lt;p&gt;Let’s focus on just &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}&lt;/script&gt; for now. To compute the cost gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}&lt;/script&gt; we simply use the chain rule.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\ \frac{\partial J}{\partial \vec{a}}\frac{\partial \vec{a}}{\partial \vec{z}}\frac{\partial \vec{z}}{\partial \vec{w}}&lt;/script&gt;

&lt;p&gt;The gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{a}}&lt;/script&gt; (can also be called a Jacobian, because it is) depends on the choice of the cost function because we can’t do anything if we haven’t picked what function to use for &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;. Also, &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{a}}{\partial \vec{z}}&lt;/script&gt; depends on the choice of activation function, although we can solve it for an arbitrary function.&lt;/p&gt;

&lt;p&gt;But for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{z}}{\partial \vec{w}}&lt;/script&gt;, we know that preactivation (&lt;script type=&quot;math/tex&quot;&gt;\vec{z}&lt;/script&gt;), at least for one neuron, will always be a simple linear combination of the parameters and the input data:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{z}=\vec{w}\mathbf{X}+\vec{b}&lt;/script&gt;

&lt;p&gt;This is also always true in standard feedforward neural networks (a.k.a. multilayer perceptron), but not so for every flavour of neural networks (e.g. convolutional neural networks have a convolution operation instead of a multiplication between &lt;script type=&quot;math/tex&quot;&gt;\vec{w}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;Before we move any further, it’s important you understand what Jacobians are. In a nutshell, the Jacobian of a vector-valued function (a function that returns a vector), which is what we are working with here, is a matrix that contains all of the function’s first order partial derivatives. It is the way to properly characterize the partial derivatives of a vector function with respect to all its input variables.&lt;/p&gt;

&lt;p&gt;If you were not already familiar with Jacobians or still unclear of what it is, I found &lt;a href=&quot;https://www.youtube.com/watch?v=bohL918kXQk&quot; target=&quot;_blank&quot;&gt;this video&lt;/a&gt; that should help (or just search for “Jacobian matrix” on YouTube and you’ll see many great introductory videos).&lt;/p&gt;

&lt;p&gt;Our Jacobians in matrix representation are as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial J}{\partial \vec{w}}=\left[\begin{matrix}\frac{\partial J}{\partial w_1}&amp;\frac{\partial J}{\partial w_2}&amp;\cdots&amp;\frac{\partial J}{\partial w_n}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial J}{\partial \vec{a}}=\left[\begin{matrix}\frac{\partial J}{\partial a_1}&amp;\frac{\partial J}{\partial a_2}&amp;\cdots&amp;\frac{\partial J}{\partial a_m}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \vec{a}}{\partial \vec{z}}=\left[\begin{matrix}\frac{\partial a_1}{\partial z_1}&amp;\frac{\partial a_1}{\partial z_2}&amp;\cdots&amp;\frac{\partial a_1}{\partial z_m}\\\frac{\partial a_2}{\partial z_1}&amp;\frac{\partial a_2}{\partial z_2}&amp;\cdots&amp;\frac{\partial a_2}{\partial z_m}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial a_m}{\partial z_1}&amp;\frac{\partial a_m}{\partial z_2}&amp;\cdots&amp;\frac{\partial a_m}{\partial z_m}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \vec{z}}{\partial \vec{w}}=\left[\begin{matrix}\frac{\partial z_1}{\partial w_1}&amp;\frac{\partial z_1}{\partial w_2}&amp;\cdots&amp;\frac{\partial z_1}{\partial w_n}\\\frac{\partial z_2}{\partial w_1}&amp;\frac{\partial z_2}{\partial w_2}&amp;\cdots&amp;\frac{\partial z_2}{\partial w_n}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial z_m}{\partial w_1}&amp;\frac{\partial z_m}{\partial w_2}&amp;\cdots&amp;\frac{\partial z_m}{\partial w_n}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where their shapes are: $ \frac{\partial J}{\partial \vec{w}} $ is $ 1 $-by-$ n $, $ \frac{\partial J}{\partial \vec{a}} $ is $ 1 $-by-$ m $, $ \frac{\partial \vec{a}}{\partial \vec{z}} $ is $ m $-by-$ m $, and $ \frac{\partial \vec{z}}{\partial \vec{w}} $ is $ m $-by-$ n $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The shapes show us that matrix multiplication present in the chain rule expansion is valid.&lt;/p&gt;

&lt;p&gt;From the above equation for &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, we can immediately compute the Jacobian &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{z}}{\partial \vec{w}}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can observe that the Jacobian &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{z}}{\partial \vec{w}}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; matrix. But at this stage, our Jacobian hasn’t given us anything useful because we still need the solution for each element of the matrix.&lt;/p&gt;

&lt;p&gt;We’ll solve an arbitrary element of the Jacobian and extend the pattern to the rest. Let’s begin.&lt;/p&gt;

&lt;p&gt;We pick an element &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial z_j}{\partial w_i}&lt;/script&gt; from the matrix, and immediately we observe that we have already encountered the generalized elements &lt;script type=&quot;math/tex&quot;&gt;z_j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; in the following equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_j=w_1\ \cdot x_{1,j}+w_2\ \cdot x_{2,j}+\ldots+w_n\ \cdot x_{n,j}+w_0=\sum_{i=0}^{n}{w_i\ \cdot x_{i,j}}&lt;/script&gt;

&lt;p&gt;Therefore:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial z_j}{\partial w_i}=\frac{\partial\left(\sum_{i=0}^{n}{w_i\ \cdot x_{i,j}}\right)}{\partial w_i}&lt;/script&gt;

&lt;p&gt;The above is a partial derivative w.r.t. &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;, so we temporarily consider &lt;script type=&quot;math/tex&quot;&gt;x_{i,j}&lt;/script&gt; to be a constant.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial z_j}{\partial w_i}=\frac{\partial\left(\sum_{i=0}^{n}{w_i\ \cdot x_{i,j}}\right)}{\partial w_i}=x_{i,j}&lt;/script&gt;

&lt;p&gt;(If it’s unclear how the above worked out, expand out the summation and do the derivatives term by term, and keep in mind that &lt;script type=&quot;math/tex&quot;&gt;x_{i,j}&lt;/script&gt; is considered to be constant, because this is a partial differentiation w.r.t. &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;We substitute the result back into the Jacobian:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \vec{z}}{\partial \vec{w}}=\left[\begin{matrix}x_{1,1}&amp;x_{2,1}&amp;\cdots&amp;x_{n,1}\\x_{1,2}&amp;x_{2,2}&amp;\cdots&amp;x_{n,2}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_{1,m}&amp;x_{2,m}&amp;\cdots&amp;x_{n,m}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Recall that we originally defined &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=\left[\begin{matrix}x_{1,1}&amp;x_{1,2}&amp;\cdots&amp;x_{1,m}\\x_{2,1}&amp;x_{2,2}&amp;\cdots&amp;x_{2,m}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_{n,1}&amp;x_{n,2}&amp;\cdots&amp;x_{n,m}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, we observe that &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{z}}{\partial \vec{w}}&lt;/script&gt; is exactly the transpose of our original definition of X:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \vec{z}}{\partial \vec{w}}= \mathbf{X}^T&lt;/script&gt;

&lt;p&gt;One Jacobian is down. Two more to go.&lt;/p&gt;

&lt;p&gt;The Jacobian &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{a}}{\partial \vec{z}}&lt;/script&gt; depends on the choice of activation function, since it is obviously the gradient of the activation w.r.t. to preactivation (i.e. the derivative of the activation function). We cannot characterize it until we fully characterize the equation for &lt;script type=&quot;math/tex&quot;&gt;\vec{a}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s go with the logistic activation function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{a}=\frac{1}{1+e^{-\vec{z}}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \vec{a}}{\partial \vec{z}}=\left[\begin{matrix}\frac{\partial a_1}{\partial z_1}&amp;\frac{\partial a_1}{\partial z_2}&amp;\cdots&amp;\frac{\partial a_1}{\partial z_m}\\\frac{\partial a_2}{\partial z_1}&amp;\frac{\partial a_2}{\partial z_2}&amp;\cdots&amp;\frac{\partial a_2}{\partial z_m}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\frac{\partial a_m}{\partial z_1}&amp;\frac{\partial a_m}{\partial z_2}&amp;\cdots&amp;\frac{\partial a_m}{\partial z_m}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;We follow the same steps as done with the first Jacobian.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{\partial\left(\frac{1}{1+e^{-z_k}}\right)}{\partial z_j}&lt;/script&gt;

&lt;p&gt;The reason for &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; is that we need a subscript that conveys the idea that &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{a}}{\partial \vec{z}}&lt;/script&gt; may not always have matching subscripts That is, we are considering all the elements of the Jacobian and not just the ones along the diagonal, which are the only elements that will have matching subscripts. However, both subscripts, &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, are tracking the same quantity, which is datapoints.&lt;/p&gt;

&lt;p&gt;Let’s rearrange the activation function a little by multiplying both numerator and denominator by &lt;script type=&quot;math/tex&quot;&gt;e^z_k&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{\partial\left(\frac{1}{1+e^{-z_k}}\cdot\frac{e_k^z}{e_k^z}\right)}{\partial z_j}=\frac{\partial\left(\frac{e_k^z}{e_k^z+1}\right)}{\partial z_j}&lt;/script&gt;

&lt;p&gt;The reason for this is to make the use of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quotient_rule&quot; target=&quot;_blank&quot;&gt;quotient rule of differentiation&lt;/a&gt; for solving the derivative easier to work with.&lt;/p&gt;

&lt;p&gt;We have to consider two possible cases. One is where &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; are equal, e.g. &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial a_2}{\partial z_2}&lt;/script&gt;, and the other is when they are not, e.g. &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial a_1}{\partial z_2}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;k\neq j&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{\partial\left(\frac{e^{z_k}}{e^{z_k}+1}\right)}{\partial z_j}=0&lt;/script&gt;

&lt;p&gt;If it’s unclear how the above worked out, then notice that when &lt;script type=&quot;math/tex&quot;&gt;k\neq j&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;z_k&lt;/script&gt; is temporarily a constant because we are differentiating w.r.t. &lt;script type=&quot;math/tex&quot;&gt;z_j&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;k=j&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{\partial\left(\frac{e^{z_k}}{e^{z_k}+1}\right)}{\partial z_k}&lt;/script&gt;

&lt;p&gt;We apply the quotient rule of differentiation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{\partial\left(\frac{e^{z_k}}{e^{z_{k}}+1}\right)}{\partial z_k}=\frac{e^{z_k}\left(e^{z_k}+1\right)-\left(e^{z_k}\right)^2}{\left(e^{z_k}+1\right)^2}&lt;/script&gt;

&lt;p&gt;We can sort of see the original activation function somewhere in there, so we rearrange the terms and see if we can get something more compact:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{e^{z_k}\cdot\left(e^{z_k}+1\right)-\left(e^{z_k}\right)^2}{\left(e^{z_k}+1\right)^2}=\frac{\left(e^{z_k}\right)^2+e^{z_k}-\left(e^{z_k}\right)^2}{\left(e^{z_k}+1\right)^2}= \color{magenta}{\frac{e^{z_k}}{e^{z_k}+1}} \cdot\left(\frac{1}{e^{z_k}+1}\right)&lt;/script&gt;

&lt;p&gt;Now we clearly see the original activation function in there (in &lt;font color=&quot;magenta&quot;&gt;magenta&lt;/font&gt;). But the other term also looks very similar, so we rework it a little more:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=\frac{e^{z_k}}{e^{z_k}+1}\cdot\left(\frac{1}{e^{z_k}+1}\right)=\color{magenta}{\frac{e^{z_k}}{e^{z_k}+1}}\cdot\left(1-\color{magenta}{\frac{e^{z_k}}{e^{z_k}+1}}\right)&lt;/script&gt;

&lt;p&gt;We can now simply substitute it in the activation (while recalling that &lt;script type=&quot;math/tex&quot;&gt;k\ =\ j&lt;/script&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial a_k}{\partial z_j}=a_k\cdot\left(1-a_k\right)=a_j\cdot\left(1-a_j\right)&lt;/script&gt;

&lt;p&gt;Therefore, our Jacobian becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \vec{a}}{\partial \vec{z}}=\left[\begin{matrix}a_1\cdot\left(1-a_1\right)&amp;0&amp;\cdots&amp;0\\0&amp;a_2\cdot\left(1-a_2\right)&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;a_m\cdot\left(1-a_m\right)\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;It’s an &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; diagonal matrix.&lt;/p&gt;

&lt;p&gt;Two Jacobians are down and one more to go.&lt;/p&gt;

&lt;p&gt;However, I will leave the details for the last Jacobian &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{a}}&lt;/script&gt; as an exercise for you (it’s not more challenging than the other two). Here’s the setup for it.&lt;/p&gt;

&lt;p&gt;The cost gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{a}}&lt;/script&gt; depends on the choice of the cost function since it is obviously the gradient of the cost w.r.t. activation. Since we are using a logistic activation function, we will go ahead and use the logistic loss function (a.k.a. cross entropy loss or negative log-likelihoods):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J=-\frac{1}{m}\cdot\sum_{j}^{m}{y_j\cdot l o g{(a}_j)+(1-y_j)\cdot\log({1-a}_j)}&lt;/script&gt;

&lt;p&gt;The result for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{a}}&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{a}}=-\frac{1}{m}\cdot\left(\frac{ \vec{y}}{\vec{a}}-\frac{1-\vec{y}}{1-\vec{a}}\right)&lt;/script&gt;

&lt;p&gt;Note that all the arithmetic operations in the above are all elementwise. The resulting cost gradient is a vector that has same shape as &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, which is &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Now we recombine everything. Therefore, the equation for computing the cost gradient for an artificial neuron that uses a logistic activation function and a cross entropy loss is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\ \frac{\partial J}{\partial \vec{a}}\frac{\partial \vec{a}}{\partial \vec{z}}\frac{\partial \vec{z}}{\partial \vec{w}}=-\frac{1}{m}\cdot\left(\frac{\vec{y}}{\vec{a}}-\frac{1-\vec{y}}{1-\vec{a}}\right)\frac{\partial \vec{a}}{\partial \vec{z}}\mathbf{X}^T&lt;/script&gt;

&lt;p&gt;We choose to combine the first two gradients into &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{z}}&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\ \frac{\partial J}{\partial \vec{z}}\mathbf{X}^T&lt;/script&gt;

&lt;p&gt;The gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{z}}&lt;/script&gt; came from this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial\vec{z}}=\frac{\partial J}{\partial\vec{a}}\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt;

&lt;p&gt;We already have everything for  &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{z}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial J}{\partial \vec{z}}=\color{brown}{\frac{\partial J}{\partial \vec{a}}}\color{blue}{\frac{\partial \vec{a}}{\partial \vec{z}}}=\color{brown}{-\frac{1}{m}\cdot\left(\frac{ \vec{y}}{ \vec{a}}-\frac{1- \vec{y}}{1- \vec{a}}\right) }\color{blue}{\left[\begin{matrix}a_1\cdot\left(1-a_1\right)&amp;0&amp;\cdots&amp;0\\0&amp;a_2\cdot\left(1-a_2\right)&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\cdots&amp;a_m\cdot\left(1-a_m\right)\\\end{matrix}\right]} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\frac{\partial J}{\partial\vec{z}}\mathbf{X}^T=\frac{\partial J}{\partial\vec{a}}\frac{\partial\vec{a}}{\partial \vec{z}}\mathbf{X}^T&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        
Where $ \frac{\partial J}{\partial \vec{w}} $ is $ 1 $-by-$ n $, $ \frac{\partial J}{\partial \vec{z}} $ is $ 1 $-by-$ m $, $ \frac{\partial J}{\partial\vec{a}} $ is a $ 1 $-by-$ m $ vector,  $ \frac{\partial\vec{a}}{\partial \vec{z}} $ is an $ m $-by-$ m $ matrix. Note that division between vectors or matrices, e.g. $ \frac{\vec{y}}{\vec{a}} $, are always elementwise.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Notice that everything needed for computing the vital cost gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}&lt;/script&gt; has either already been computed during forward propagation or is from the data. We are simply reusing values already computed prior.&lt;/p&gt;

&lt;p&gt;The above equation can now be easily implemented in code in a vectorized fashion. Implementing the code for computing the gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; in a vectorized fashion is a little tricky. To compute it, we first compute its diagonal as a row vector:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;diagonal\ vector\ of\ \frac{\partial\vec{a}}{\partial \vec{z}}=(\vec{a}\odot\left(1-\vec{a}\right))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
=\left[\begin{matrix}a_1\cdot(1-a_1\ )&amp;a_2\cdot(1-a_2\ )&amp;\cdots&amp;a_m\cdot(1-a_m\ )\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \vec{a} $ is the $ 1 $-by-$ m $ vector that contains the activations. The symbol $ \odot $ represents elementwise multiplication (a.k.a. Hadamard product).
&lt;br /&gt;&lt;br /&gt;
The $ diagonal\ vector\ of\ \frac{\partial\vec{a}}{\partial \vec{z}} $ is the $ 1 $-by-$ m $ vector that you will obtain if you pulled out the diagonal of the matrix $ \frac{\partial\vec{a}}{\partial \vec{z}} $ and put it into a row vector.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We also observe that the &lt;script type=&quot;math/tex&quot;&gt;diagonal\ vector\ of\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; (the vector that you get if you pulled out the diagonal of the matrix &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; and put it into a row vector) is simply the elementwise derivative of the vector &lt;script type=&quot;math/tex&quot;&gt;\vec{z}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
diagonal\ vector\ of\frac{\partial\vec{a}}{\partial\vec{z}}=\left[\begin{matrix}a_1\cdot\left(1-a_1\ \right)&amp;a_2\cdot\left(1-a_2\ \right)&amp;\cdots&amp;a_m\cdot\left(1-a_m\ \right)\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
=\ \left[\begin{matrix}f'(z_1)&amp;f'(z_2)&amp;\cdots&amp;f'(z_m)\\\end{matrix}\right]=f'(\vec{z}) %]]&gt;&lt;/script&gt;

&lt;p&gt;So, computing the &lt;script type=&quot;math/tex&quot;&gt;diagonal\ vector\ of\frac{\partial\vec{a}}{\partial\vec{z}}&lt;/script&gt; is simply same as computing &lt;script type=&quot;math/tex&quot;&gt;f'(\vec{z})&lt;/script&gt;, and this applies to any activation function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and its derivative &lt;script type=&quot;math/tex&quot;&gt;f'&lt;/script&gt;. And this is easily implemented in code.&lt;/p&gt;

&lt;table&gt;
&lt;td&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;b&gt;Why the $ diagonal\ vector\ of\frac{\partial\vec{a}}{\partial\vec{z}} $ is always equal to $ f'(\vec{z}) $ for any activation function:
&lt;/b&gt;
&lt;/summary&gt;
&lt;p&gt;
The reason why the expression, $ diagonal\ vector\ of\frac{\partial\vec{a}}{\partial\vec{z}}=f\prime(\vec{z}) $, is valid for the logistic activation function is precisely because of this result (already shown before):

$$
\frac{\partial a_k}{\partial z_j}=\frac{\partial\left(\frac{e^{\vec{z}_\boldsymbol{k}}}{e^{\vec{z}_\boldsymbol{k}}+1}\right)}{\partial z_j}=0
$$

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        
For $ k\neq j $. Where both $ j $ and $ k $ track the same quantity, which is datapoints.
    &lt;/p&gt;
&lt;/div&gt;


The above equation tell us that the only time an element of the matrix $ \frac{\partial\vec{a}}{\partial\vec{z}} $ has a chance of being non-zero is when $ k=j $, which is the diagonal.
&lt;br /&gt;&lt;br /&gt;
The great thing is that the above equation also holds true for any activation function because the reason it results in zero for the logistic activation function has nothing to do with the activation function but simply because under the condition of $ k\neq j $, the following is also true: $ z_k\neq z_j $.
&lt;br /&gt;&lt;br /&gt;
Therefore, in general the following expression will hold true for any activation function $ f $:

$$
\frac{\partial a_k}{\partial z_j}=\frac{\partial f(z_k)}{\partial z_j}=0
$$

Which also means for any activation function $ f $, the following is also true:

$$
diagonal\ vector\ of\frac{\partial\vec{a}}{\partial\vec{z}}=f\prime(\vec{z})
$$
&lt;/p&gt;
&lt;/details&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;p&gt;Once we’ve computed the &lt;script type=&quot;math/tex&quot;&gt;diagonal\ vector\ of\ \frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt;, which is a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector, we will implement some code that can inflate the diagonal matrix &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; by padding it with zeros. If coding in Python and using the NumPy library for our vectorized computations, then the method &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.diagflat.html&quot; target=&quot;_blank&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.diagflat&lt;/code&gt;&lt;/a&gt; does exactly that.&lt;/p&gt;

&lt;p&gt;One good news is that we can take the equation &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\frac{\partial J}{\partial\vec{a}}\frac{\partial\vec{a}}{\partial \vec{z}}\mathbf{X}^T&lt;/script&gt; to an alternative form that would allow us to skip the step of inflating the &lt;script type=&quot;math/tex&quot;&gt;diagonal\ vector\ of\ \frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; and therefore saves us a little processing time.&lt;/p&gt;

&lt;p&gt;There is a well-known relationship between the multiplication of a vector with a diagonal matrix, and elementwise multiplication (a.k.a. Hadamard product), which is denoted as &lt;script type=&quot;math/tex&quot;&gt;\odot&lt;/script&gt;. The relationship plays out like this.&lt;/p&gt;

&lt;p&gt;Say we have a row vector &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; and a diagonal matrix &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, and when we flatten the &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; into a row vector &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; (that is, we pull out the diagonal from &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt; and put it into a row vector), whose elements is just the diagonal of &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;, then we can write:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\color{brown}{v}\color{blue}{D}=\color{brown}{v} \odot \color{blue}{d}&lt;/script&gt;

&lt;p&gt;(Test out the above for yourself with small vectors and matrices and see if the two sides indeed equate to one another).&lt;/p&gt;

&lt;p&gt;We apply this relationship to our gradients and get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{z}}=\frac{\partial J}{\partial\vec{a}}\frac{\partial\vec{a}}{\partial \vec{z}}=\frac{\partial J}{\partial \vec{a}}\odot\left(diagonal\ vector\ of\ \frac{\partial\vec{a}}{\partial \vec{z}}\right)&lt;/script&gt;

&lt;p&gt;In fact, we can casually equate &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;f'(\vec{z})&lt;/script&gt;, which is same as its diagonal vector. The math works out in a very nice way in that it gives the impression that we are extracting only the useful information from the matrix (which is the diagonal of the matrix).&lt;/p&gt;

&lt;p&gt;Therefore, we end up perfoming the following assignment operation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\vec{a}}{\partial \vec{z}}:=f'(\vec{z})=(\vec{a}\odot\left(1-\vec{a}\right))&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Note that the symbol := means that this is an assignment statement, not an equation. That is, we are setting the term on the LHS to represent the terms on the RHS.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Therefore, our final equation for computing the cost gradient &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}&lt;/script&gt; can be written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\frac{\partial J}{\partial\vec{z}}\frac{\partial \vec{z}}{\partial \vec{w}}=\ \frac{\partial J}{\partial\vec{z}}\mathbf{X}^T=\frac{\partial J}{\partial\vec{a}}\odot\frac{\partial\vec{a}}{\partial \vec{z}}\mathbf{X}^T=\frac{\partial J}{\partial\vec{a}}\odot f'(\vec{z})\mathbf{X}^T&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=-\frac{1}{m}\bullet\left(\frac{\vec{y}}{\vec{a}}-\frac{1-\vec{y}}{1-\vec{a}}\right)\ \odot(a\odot\left(1-a\right))\mathbf{X}^T&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        
Where $ \frac{\partial\vec{a}}{\partial\vec{z}} $ here is just the diagonal of the actual $ \frac{\partial\vec{a}}{\partial\vec{z}} $ and has a shape of $ 1 $-by-$ m $ and is equal to $ f'(\vec{z}) $.
&lt;br /&gt;&lt;br /&gt;
Note that we applied a property of how Hadamard product interacts with matrix multiplication: $ \left(v \odot u\right)M = v\odot uM = \left(u \odot v\right)M=u\odot vM $. Where $ v $ and $ u $ are vectors of same length, and $ M $ is a matrix for which the matrix multiplication shown are valid.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt;, we can borrow a lot of what we did for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial \vec{w}}&lt;/script&gt; here as well.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\frac{\partial J}{\partial\vec{z}}\ \frac{\partial\vec{z}}{\partial b}=\frac{\partial J}{\partial\vec{a}}\frac{\partial\vec{a}}{\partial \vec{z}}\frac{\partial \vec{z}}{\partial b}&lt;/script&gt;

&lt;p&gt;We know that &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt; has to be a scalar (or &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; vector) because there is only one bias in the model, unlike weights, of which there are &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; of them. During gradient descent, there is only one bias value to update, so if we have a vector or matrix for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt;, then we won’t know what to do with all those values in the vector or matrix.&lt;/p&gt;

&lt;p&gt;We have to recall that the only reason that &lt;script type=&quot;math/tex&quot;&gt;\vec{b}&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector in the equations for forward propagation is because it gets stretched (broadcasted) into a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector to match the shape of &lt;script type=&quot;math/tex&quot;&gt;\vec{z}&lt;/script&gt;, so that the equations are valid. Fundamentally, it is a scalar and so is &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Although the further breakdown of &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{z}}&lt;/script&gt; into &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{a}}\frac{\partial\vec{a}}{\partial \vec{z}}&lt;/script&gt; is shown above, we won’t need to use that since we already fully delineated &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{z}}&lt;/script&gt; earlier. So, we just tackle &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{z}}\frac{\partial\vec{z}}{\partial b}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Actually, just need &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{z}}{\partial b}&lt;/script&gt; since we already have &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial\vec{z}}&lt;/script&gt;. The matrix representation of &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{z}}{\partial b}&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\vec{z}}{\partial b}=\left[\begin{matrix}\frac{\partial z_1}{\partial b}\\\frac{\partial z_2}{\partial b}\\\vdots\\\frac{\partial z_m}{\partial b}\\\end{matrix}\right]\&lt;/script&gt;

&lt;p&gt;Let’s work on it but keeping things in compact format:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\vec{z}}{\partial b}=\frac{\partial(\vec{w}\mathbf{X} +\ \vec{b})}{\partial b}=\frac{\partial(\vec{w}\mathbf{X})}{\partial b}+\frac{\partial\vec{b}}{\partial b}=0+\frac{\partial\vec{b}}{\partial b}=\frac{\partial\vec{b}}{\partial b}&lt;/script&gt;

&lt;p&gt;Let’s examine &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{b}}{\partial b}&lt;/script&gt;. It’s an m-by-1 vector that is equal to &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{z}}{\partial b}&lt;/script&gt;, which also means it has same shape as &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{z}}{\partial b}&lt;/script&gt;. You also observe that it has the shape of &lt;script type=&quot;math/tex&quot;&gt;\vec{z}^T&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;When you transpose a vector or matrix, you also transpose their shape, which fortunately is simply done by reversing the order of the shape, so when a 1-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector is transposed, its new shape is &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;-by-1. And note that the content of &lt;script type=&quot;math/tex&quot;&gt;\vec{b}&lt;/script&gt; is just &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; repeating &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; times. So, &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{b}}{\partial b}&lt;/script&gt; looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\vec{b}}{\partial b}=\left[\begin{matrix}\frac{\partial b}{\partial b}\\\frac{\partial b}{\partial b}\\\vdots\\\frac{\partial b}{\partial b}\\\end{matrix}\right]=\left[\begin{matrix}1\\1\\\vdots\\1\\\end{matrix}\right]\&lt;/script&gt;

&lt;p&gt;Therefore &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{z}}{\partial\boldsymbol{b}}&lt;/script&gt; is a vector of all ones that has the shape &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; (the shape of &lt;script type=&quot;math/tex&quot;&gt;\vec{z}^T&lt;/script&gt;).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\vec{z}}{\partial b}=\frac{\partial\vec{b}}{\partial b}=\left[\begin{matrix}\frac{\partial b}{\partial b}\\\frac{\partial b}{\partial b}\\\vdots\\\frac{\partial b}{\partial b}\\\end{matrix}\right]=\left[\begin{matrix}1\\1\\\vdots\\1\\\end{matrix}\right]&lt;/script&gt;

&lt;p&gt;Thus &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt; is fully delineated:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\frac{\partial J}{\partial\vec{z}}\ \frac{\partial\vec{z}}{\partial b}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \frac{\partial J}{\partial\vec{z}} $ is the gradient already computed in the steps for computing $ \frac{\partial J}{\partial\vec{w}} $, and $ \frac{\partial \vec{z}}{\partial b} $ is an $ m $-by-$ 1 $ vector of ones (i.e. has same shape as $ \vec{z}^T $).
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Therefore the Jacobian &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{z}}{\partial b}&lt;/script&gt; is easily implemented in code by simply creating a vector of ones whose shape is same as $ \vec{z}^T $. But there is another way we can recharacterize the above equation for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt; such that we avoid creating any new vectors.&lt;/p&gt;

&lt;p&gt;As &lt;a href=&quot;/understand-an-artificial-neuron-from-scratch.html#artificial-neuron&quot; target=&quot;_blank&quot;&gt;mentioned in part 2&lt;/a&gt;, matrix multiplication, or specifically vector-matrix multiplication, is essentially one example of tensor contraction.&lt;/p&gt;

&lt;p&gt;Below is a quick overview of tensor contraction.&lt;/p&gt;

&lt;p&gt;Before continuing, note that there is a whole world of concepts associated with tensors and their contraction that is far beyond the scope of this blog series. We will go over just what we need. You can liken the overview presented here to talking about simple linear regression when an overview of machine learning is promised. Let’s continue!&lt;/p&gt;

&lt;p&gt;From the perspective of tensor contraction, using an elementwise notation, the vector-matrix multiplication of a row vector &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; and a matrix &lt;script type=&quot;math/tex&quot;&gt;\mathbf{M}&lt;/script&gt; to produce a row vector &lt;script type=&quot;math/tex&quot;&gt;\vec{u}&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_q=\sum_{p}{v_p\cdot m_{p,q}}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where the subscript $ p $ tracks the only non-unit axis of the vector $ \vec{v} $, and the subscript $ q $ tracks second axis of the matrix $ \mathbf{M} $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;It shows exactly the elementwise version of matrix multiplication. Here is an example to illustrate the above. Say that &lt;script type=&quot;math/tex&quot;&gt;\vec{v}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{M}&lt;/script&gt; are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
v=\ \left[\begin{matrix}1&amp;2\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
M=\left[\begin{matrix}3&amp;5&amp;7\\4&amp;6&amp;8\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        The vector $ v $ is $ 1 $-by-$ 2 $, and we will use the subscript $ q $ to track the non-unit axis, i.e. the second axis (the one that counts to a maximum of 2). That is: $ v_1=1 $ and $ v_2=2 $.
&lt;br /&gt;&lt;br /&gt;
The matrix $ M $ is $ 2 $-by-$ 3 $, and we will use the subscript $ q $ to track the first axis (the one that counts to a maximum of 2) and $ p $ to track the second axis (the one that counts to a maximum of 3). That is $ m_{2,1}=4 $ and $ m_{1,3}=7 $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We know that the vector-matrix multiplication, &lt;script type=&quot;math/tex&quot;&gt;\vec{v}\mathbf{M}&lt;/script&gt;, produces a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt; vector. Let’s call it &lt;script type=&quot;math/tex&quot;&gt;\vec{u}&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{u}=\left[\begin{matrix}u_1&amp;u_2&amp;u_3\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Using the tensor contraction format, we can fully characterize what the resulting vector &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; is, by describing it elementwise:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_q=\sum_{p}{v_p\cdot M_{p,q}}&lt;/script&gt;

&lt;p&gt;For instance,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_1=v_1\cdot m_{1,1}+v_2\cdot m_{2,1}=1\cdot3+2\cdot4=11&lt;/script&gt;

&lt;p&gt;And we can do this for &lt;script type=&quot;math/tex&quot;&gt;u_2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;u_3&lt;/script&gt; (try it). In all, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
u=\left[\begin{matrix}11&amp;17&amp;23\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;To summarize, the vector multiplication &lt;script type=&quot;math/tex&quot;&gt;vM&lt;/script&gt; is a contraction along the axis tracked by subscript &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can use the tensor contraction format to recharacterize our solution for $ \frac{\partial J}{\partial b} $.&lt;/p&gt;

&lt;p&gt;In tensor contraction format, this equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\frac{\partial J}{\partial\vec{z}}\ \frac{\partial\vec{z}}{\partial b}&lt;/script&gt;

&lt;p&gt;Can be written as this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\sum_{j=1}^{m}{\left(\frac{\partial J}{\partial\vec{z}}\right)_j\cdot\left(\frac{\partial\vec{b}}{\partial\vec{b}}\right)_j}&lt;/script&gt;

&lt;p&gt;And because &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial\vec{b}}{\partial b}&lt;/script&gt; is a vector of ones, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\sum_{j=1}^{m}\left(\frac{\partial J}{\partial\vec{z}}\right)_j&lt;/script&gt;

&lt;p&gt;In essence, we just summed across the second axis of $ \frac{\partial J}{\partial \vec{z}} $ which reduced it to a $1$-by-$1$ vector that is equal to $ \frac{\partial J}{\partial b} $.&lt;/p&gt;

&lt;p&gt;We now have all our cost gradients fully delineated.&lt;/p&gt;

&lt;p&gt;In all, we can summarize with this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial b}=\frac{\partial J}{\partial\vec{z}}\ \frac{\partial\vec{z}}{\partial b}=\sum_{j=1}^{m}\left(\frac{\partial J}{\partial\vec{z}}\right)_j&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \frac{\partial J}{\partial\vec{z}} $ is the gradient already computed in the steps for computing $ \frac{\partial J}{\partial\vec{w}} $, and $ \frac{\partial \vec{z}}{\partial b} $ is an $ m $-by-$ 1 $ vector of ones (i.e. has same shape as $ \vec{z}^T $).
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Although, we don’t really need to see the equation for $ \frac{\partial J}{\partial \vec{w}} $ in its contraction format, we will present it for the sake of it. We already know that $ \frac{\partial J}{\partial \vec{w}} $ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J}{\partial \vec{w}}=\ \frac{\partial J}{\partial \vec{z}}\mathbf{X}^T&lt;/script&gt;

&lt;p&gt;And we also already know that $ \frac{\partial J}{\partial \vec{z}} $ is a $ 1 $-by-$ m $ row vector and $ X $ is an $ n $-by-$ m $ matrix, which makes $ \mathbf{X}^T $ an $ m $-by-$ n $ matrix. In tensor contraction format, the above equation is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(\frac{\partial J}{\partial\vec{w}}\right)_i=\sum_{j=1}^{m}{\left(\frac{\partial J}{\partial\vec{z}}\right)_j\cdot\left(\mathbf{X}^T\right)_{j,i}}&lt;/script&gt;

&lt;p&gt;If you singled out a feature from your data (i.e. a column from &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}^T&lt;/script&gt;) and replaced all of its values for all datapoints with 1, the above equation will turn exactly into the tensor contraction of the equation for &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial J}{\partial b}&lt;/script&gt;. This is exactly in line with what the bias node represents.&lt;/p&gt;

&lt;p&gt;The next step is to now implement these equations in code.&lt;/p&gt;</content><author><name>Prince Okoli</name></author><summary type="html"></summary></entry><entry><title type="html">Catching AI with its pants down: Understand an Artificial Neuron from Scratch</title><link href="http://localhost:4000/understand-an-artificial-neuron-from-scratch.html" rel="alternate" type="text/html" title="Catching AI with its pants down: Understand an Artificial Neuron from Scratch" /><published>2020-03-20T00:00:00-06:00</published><updated>2020-03-20T00:00:00-06:00</updated><id>http://localhost:4000/understand-an-artificial-neuron-from-scratch</id><content type="html" xml:base="http://localhost:4000/understand-an-artificial-neuron-from-scratch.html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
          }
        });
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;
&lt;/script&gt;

&lt;table&gt;
&lt;td&gt;
&lt;i&gt;We will strip the mighty, massively hyped, highly dignified AI of its cloths, and bring its innermost details down to earth!&lt;/i&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#prologue&quot; id=&quot;markdown-toc-prologue&quot;&gt;&lt;strong&gt;Prologue&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-brain-as-a-function&quot; id=&quot;markdown-toc-the-brain-as-a-function&quot;&gt;&lt;strong&gt;The brain as a function&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#biological-neuron&quot; id=&quot;markdown-toc-biological-neuron&quot;&gt;&lt;strong&gt;Biological neuron&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#toy-dataset-for-this-blog-series&quot; id=&quot;markdown-toc-toy-dataset-for-this-blog-series&quot;&gt;&lt;strong&gt;Toy dataset for this blog series&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#artificial-neuron&quot; id=&quot;markdown-toc-artificial-neuron&quot;&gt;&lt;strong&gt;Artificial neuron&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#activation-functions&quot; id=&quot;markdown-toc-activation-functions&quot;&gt;&lt;strong&gt;Activation functions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#loss-function&quot; id=&quot;markdown-toc-loss-function&quot;&gt;&lt;strong&gt;Loss function&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prologue&quot;&gt;&lt;strong&gt;Prologue&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;This is part 2 of this blog series, &lt;em&gt;Catching AI with its pants down&lt;/em&gt;, which aims to explore the inner workings of neural networks and show how to biuld a standard feedforward neural network from scratch. In this part, I will go over the biological inspiration for the artificial neuron and its mathematical underpinnings.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parts&lt;/th&gt;
      &lt;th&gt;Catching AI with its pants down&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/some-musings-about-ai.html&quot;&gt;&lt;strong&gt;Some Musings About AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 2&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/understand-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Understand an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 3&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/optimize-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Optimize an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 4&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/implement-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Implement an artificial neuron from scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 5&lt;/td&gt;
      &lt;td&gt;Understand a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 6&lt;/td&gt;
      &lt;td&gt;Optimize a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 7&lt;/td&gt;
      &lt;td&gt;Implement a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;the-brain-as-a-function&quot;&gt;&lt;strong&gt;The brain as a function&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The computational theory of mind (CTM) says that we can interpret human cognitive processes as computational functions. That is, the human mind behaves just like a computer.&lt;/p&gt;

&lt;p&gt;Note that while CTM is considered a decent model for human cognition (it was the unchallenged standard in the 1960s and 1970s and still widely subscribed to), no one has been able to show how consciousness can emerge from a system modelled on the basis of this theory, but that’s another topic for another time.&lt;/p&gt;

&lt;p&gt;For a short primer on CTM, see &lt;a href=&quot;https://plato.stanford.edu/entries/computational-mind/&quot; target=&quot;_blank&quot;&gt;this article&lt;/a&gt; from the Stanford Encyclopedia of Philosophy.&lt;/p&gt;

&lt;p&gt;According to CTM, if we have a mathematical model of all the computations that goes on in the brain, we should, one day, be able to replicate the capabilities of the brain with computers. But how does the brain do what it does?&lt;/p&gt;

&lt;h3 id=&quot;biological-neuron&quot;&gt;&lt;strong&gt;Biological neuron&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In a nutshell, the brain is made up of two main kinds of cells: glial cells and neurons (a.k.a. nerve cells). There are about &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2776484/&quot; target=&quot;_blank&quot;&gt;86 billion neurons&lt;/a&gt; and even more glial cells in the nervous system (brain, spinal cord and nerves) of an adult human. The primary function of glial cells is to provide physical protection and other kinds of support to neurons, so we are not very interested in glial cells here. It’s the neuron we came for.&lt;/p&gt;

&lt;p&gt;The primary function of biological neurons is to process and transmit signals, and there are three main types, sensory neurons (concentrated in your sensory organs like eyes, ears, skin, etc.), motor neurons (carry signals between the brain and spinal cord, and from both to the muscles), and interneurons (found only in the brain and spinal cord, and they process information).&lt;/p&gt;

&lt;p&gt;For instance, when you grab a very hot cup, sensory neurons in the nerves of your fingers send a signal to interneurons in your spinal cord. Some interneurons pass the signal on to motor neurons in your hand, which causes you to drop the cup, while other interneurons send a signal to those in your brain, and you experience pain.&lt;/p&gt;

&lt;p&gt;So clearly, in order to start modelling the brain, we have to first understand the neuron and try to model it mathematically.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/biological_neuron.png&quot; alt=&quot;A biological neuron is the building block of the nervous system, which includes the brain. Source: &amp;lt;a href='https://cdn.kastatic.org/ka-perseus-images/3567fc3560de474001ec0dafb068170d30b0c751.png'&amp;gt;Khan Academy&amp;lt;/a&amp;gt;.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;A biological neuron is the building block of the nervous system, which includes the brain. Source: &lt;a href=&quot;https://cdn.kastatic.org/ka-perseus-images/3567fc3560de474001ec0dafb068170d30b0c751.png&quot;&gt;Khan Academy&lt;/a&gt;.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Neurons in the brain usually work in groups known as neural circuits (or biological neural networks), where they provide some biological function. A neuron has 3 main parts: the dendrites, soma (cell body) and axon.&lt;/p&gt;

&lt;p&gt;The dendrite of one neuron is connected to the axon terminal of another neuron, and so on, resulting in a network of connected neurons. The connection between two neurons is known as the synapse, and there is no actual physical contact, as the neurons don’t actually touch each other.&lt;/p&gt;

&lt;p&gt;Instead, a neuron will release chemicals (neurotransmitters) that carry the electrical signal to the dendrite of the next neuron. The strength of the transmission is known as the synaptic strength. The more often signals are transmitted across a synapse, the stronger the synaptic strength becomes. This rule, commonly known as Hebb’s rule (introduced in 1949 by Donald Hebb), is colloquially stated as, “neurons that fire together wire together.”&lt;/p&gt;

&lt;p&gt;Neurons receive signal via their dendrites and outputs signal via their axon terminals. And each neuron can be connected to thousands of other neurons. When a neuron receives signals from other neurons, it combines all the input signals and generates a voltage (known as graded potential) on the membrane of the soma that is proportional, in size and duration, to the sum of the input signals.&lt;/p&gt;

&lt;p&gt;The graded potential gets smaller as it travels through the soma to reach the axon. If the graded potential that reaches the trigger zone (near the axon hillock) is higher than a threshold value unique to the neuron, the neuron fires a huge electric signal, called the action potential, that travels down the axon and through the synapse to become the input signal for the neurons downstream.&lt;/p&gt;

&lt;h2 id=&quot;toy-dataset-for-this-blog-series&quot;&gt;&lt;strong&gt;Toy dataset for this blog series&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Before we advance any further to artificial neurons, let’s introduce a toy dataset that will accompany subsequent discussions and be used to provide vivid illustration.&lt;/p&gt;

&lt;p&gt;You can think of the data as being generated from an experiment where a device launches balls of various masses unto a board that can roll backward, and when it does roll back all the way to touch the sensor, that shot is recorded as high energy, otherwise it is classified as low energy.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/toy_experiment_schematic.png&quot; alt=&quot;A schematic of the toy experiment.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;A schematic of the toy experiment.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Below is an excerpt of the dataset:&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/toy_dataset_excerpt.png&quot; alt=&quot;A few records (datapoints) from the toy dataset, showing all the features and targets (the column headings).&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;A few records (datapoints) from the toy dataset, showing all the features and targets (the column headings).&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The dataset has two features or inputs, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;velocity&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;mass&lt;/code&gt;, and a single output, which is &lt;code class=&quot;highlighter-rouge&quot;&gt;energy level&lt;/code&gt; and it is binary. The last two columns are exactly the same, just that the third is the numerical version of the last and is what we actually use because we need to crunch numbers. In classification, the labels are converted to numbers for the learning process.&lt;/p&gt;

&lt;p&gt;The full toy dataset can be found &lt;a href=&quot;https://github.com/princyok/deep_learning_without_ml_libraries/blob/master/datasets/toy_dataset1/toy_dataset_velocity_ke.csv&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;artificial-neuron&quot;&gt;&lt;strong&gt;Artificial neuron&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In the 1950s, the psychologist Frank Rosenblatt introduced a very simple mathematical abstraction of the biological neuron. He developed a model that mimicked the following behavior: signals that are received from dendrites are sent down the axon once the strength of the input signal crosses a certain threshold. The outputted signal can then serve as an input to another neuron. Rosenblatt &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/B0080430767005726&quot; target=&quot;_blank&quot;&gt;named&lt;/a&gt; this mathematical model the &lt;strong&gt;perceptron&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Rosenblatt’s original perceptron was a simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Heaviside_step_function&quot; target=&quot;_blank&quot;&gt;Heaviside function&lt;/a&gt; that outputs zero if the input signal is equal to or less than 0, and outputs 1 if the input is greater than zero. Therefore, zero was the threshold above which an input makes the neuron to fire. The original perceptron is an example of an artificial neuron, and we will see other examples.&lt;/p&gt;

&lt;p&gt;An artificial neuron is simply a mathematical function that serves as the elementary unit of a neural network. It is also known as a node or a unit, with the latter name being very common in machine learning publications. I may jump between these names, and it’s not bad if you get used to that, as all of these names are common.&lt;/p&gt;

&lt;p&gt;This mathematical function has a collection of inputs, &lt;script type=&quot;math/tex&quot;&gt;x_1,x_2,\ \ldots,\ x_n&lt;/script&gt;, and a single output, &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;, commonly known as the activation value (or post-activation value), or often without the term “value” (i.e. simply activation).&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/artificial_neuron.png&quot; alt=&quot;Diagram of an artificial neuron.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Diagram of an artificial neuron.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;But what then happens inside a unit (an artificial neuron)?&lt;/p&gt;

&lt;p&gt;The inputs that are fed into a unit are used in two key operations in order to generate the activation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Summation: The inputs ($ x_i $) are multiplied with the weights (&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;), and the products are summed together. This summation is sometimes called the preactivation value, or without the term “value”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Activation function (a.k.a. transfer function): the resulting sum (i.e. the preactivation) is passed through a mathematical function.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/artificial_neuron_interior.png&quot; alt=&quot;Diagram of an artificial neuron showing what happens inside it. This is the less common representation, as it is thought of as showing too many details you are expected to already know.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Diagram of an artificial neuron showing what happens inside it. This is the less common representation, as it is thought of as showing too many details you are expected to already know.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The activation value can be thought of as a loose adaptation of the biological action potential, and the weights imitate synaptic strength.&lt;/p&gt;

&lt;p&gt;The inputs to the neuron, &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;, can themselves be activation values from other neurons. However, at this stage, where we are focusing on the model for only one artificial neuron, we will set the inputs to be the data, which loosely represents the stimuli received by the sensory organs in the biological analogy.&lt;/p&gt;

&lt;p&gt;The algebraic representation of an artificial neuron is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a=f\left(z\right)&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ a $ is the activation, $ z $ is the preactivation, and $ f $ is the activation function (in the case of the original perceptron, this is the Heaviside function).
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The preactivation $ z $ is computed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w_1\ \cdot x_1+w_2\ \cdot x_2+\ldots+w_n\ \cdot x_n+w_0=\sum_{i=0}^{n}{w_i\ \cdot x_i}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ n $ is the number of features in our dataset, which means $ i $ tracks the features (i.e. it is the variable for the serial number of the features).
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Now check back with the diagram of an artificial neuron and see if you can make the connection between the equations and the diagram. Don’t move on unless you already have this down.&lt;/p&gt;

&lt;p&gt;It’s important to start putting these equations in the context of data. Using our toy dataset (introduced above), the application of this equation can be demonstrated by taking any datapoint and subbing the values into the above equation. For instance, if we sub in the 0&lt;sup&gt;th&lt;/sup&gt; datapoint (6.5233, 1.5484, 0), we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w_1\ \cdot 6.5233+w_2\ \cdot1.5484+w_0&lt;/script&gt;

&lt;p&gt;We will keep the weights as variables for now because we don’t know the appropriate weights for this dataset (that’s a problem we leave for when we train the artificial neuron).
The complete algebraic representation of the original perceptron, which has the Heaviside function as its activation function, is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
a =
\begin{cases}
 1 &amp;\text{if } z &gt; 0 \\
0 &amp;\text{if } z \leq 0
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;If you took a moment to really look at the equation for preactivation, you will notice something is off, compared to the artificial neuron diagram. Where did &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; come from? And what about &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;? The answer is the “bias term”. That’s the name for &lt;script type=&quot;math/tex&quot;&gt;w_0 \cdot x_0&lt;/script&gt;. It allows our function to shift, and its presence is purely a mathematical necessity.&lt;/p&gt;

&lt;p&gt;The variable &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; is known as the bias, and &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt; (commonly referred to as the bias node) is a constant that is always equal to one and has nothing to do with the data, unlike &lt;script type=&quot;math/tex&quot;&gt;x_1&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;x_n&lt;/script&gt; that comes from the data (e.g. the pixels of the images in the case of an image classification task). That’s how &lt;script type=&quot;math/tex&quot;&gt;w_0 \cdot x_0&lt;/script&gt; reduces to just &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, we will henceforth refer to &lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;, and this is the letter often used in literature to represent the bias. The weights (&lt;script type=&quot;math/tex&quot;&gt;w_1,\ w_2,\ \ldots,\ w_n&lt;/script&gt;) and bias (&lt;script type=&quot;math/tex&quot;&gt;w_0&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;) collectively are known as the parameters of the artificial neuron.&lt;/p&gt;

&lt;table&gt;
&lt;td&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;b&gt;
The need for the bias term:
&lt;/b&gt;
&lt;/summary&gt;
&lt;p&gt;
As already mentioned, the bias term allows our function to shift. Its presence is purely a mathematical necessity.
&lt;br /&gt;&lt;br /&gt;
The equation for z is a linear equation:

$$
z=w_1\ \cdot x_1+w_2\ \cdot x_2+\ldots+w_n\ \cdot x_n+w_0
$$

If we limit the number of features (input variables) to only one, we get the equation of a line:

$$
z=w_1\ \cdot x_1+w_0
$$

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ w_1 $ is the slope of the line, and $ w_0 $ is the vertical axis intercept.
    &lt;/p&gt;
&lt;/div&gt;


Everything looks good. If we are given exactly two datapoints, we will be able to perfectly fit a line through them, and we will be able to calculate the slope and intercept of that line, thereby fully solving the equation of that line. That process of solving the equation to fit the data made of two points is “learning”. In fact, feel free to call it machine learning.
&lt;br /&gt;&lt;br /&gt;
But what if we omitted the vertical axis intercept? Well, we may never be able to perfectly fit a line through those two datapoints. Actually, we will never be able to perfectly fit a straight line through both points if it happens that the line that perfectly fits on them does not go through the origin (which is intercept of zero).
&lt;br /&gt;&lt;br /&gt;
&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/line_varying_slopes.png&quot; alt=&quot;Plot of lines of various slopes (m) all passing through the origin (c=0) and compared against two datapoints that cannot be perfectly fitted by a line whose y-intercept is 0, because a vertical shift is necessary.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Plot of lines of various slopes (m) all passing through the origin (c=0) and compared against two datapoints that cannot be perfectly fitted by a line whose y-intercept is 0, because a vertical shift is necessary.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;


But by having the intercept term, we can shift the line vertically.
&lt;br /&gt;&lt;br /&gt;
In general, it goes like this: If we have a function $ f(x) $, then $ f\left(x\right)+c $ applies a vertical shift of $ c $ on the function. Whereas, $ f(x+c) $ applies a horizontal shift of $ c $. This should be enough refresher of this high school topic, and it is also the reason why we need the bias term.
&lt;br /&gt;&lt;br /&gt;
But the presence of the bias term in our artificial neuron equation means that the true diagram should look like this:
&lt;br /&gt;&lt;br /&gt;
&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/artificial_neuron_bias_node.png&quot; alt=&quot;Diagram of an artificial neuron showing the bias node.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Diagram of an artificial neuron showing the bias node.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;


But we don’t show the bias nodes because it is generally assumed that everyone should know that it is always there. This is important because it is common for the bias term to be completely omitted in many ML publications, because they know that you should know that it is there!
&lt;/p&gt;
&lt;/details&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;p&gt;We observe that the equation for an artificial neuron can be condensed into this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a=f(x;w,b)&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ x=(x_1,\ x_2,\ldots,\ x_n) $ and $ w=(w_1,\ w_2,\ldots,w_n) $
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The equation is read as $ a $ is a function of $ x $ parameterized by $ w $ and $ b $. And in fact, we’ve just introduced vectors. One geometrical interpretation of a vector in a given space (could be 2D, 3D space, etc.) is that it is a point with a “sense” of direction, or just an arrow pointing from the origin to a point.&lt;/p&gt;

&lt;p&gt;So effectively we have this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a=f(\vec{x};\vec{w},b)&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where
$
\vec{x}=\left[\begin{matrix}x_1\\x_2\\\vdots\\x_n\\\end{matrix}\right]
$
and
$
\vec{w}=\left[\begin{matrix}w_1\\w_2\\\vdots\\w_n\\\end{matrix}\right]^T
$.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;If you are doubting, then check if this equation is correct (spoiler alert: it is correct!):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{w}\ \cdot\vec{x}=\vec{w}\vec{x}=\sum_{i=1}^{n}{w_i\ \cdot x_i}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        
Note that the lack of any symbols between $ \vec{w} $ and $ \vec{x} $ signifies vector-vector multiplication, which is same as dot product of vectors. It's also common for vector-matrix multiplication and matrix-matrix multiplication to be presented the same way, because they are all kinds of matrix multiplication. The dot symbol ($ \cdot $) between any two scalars means regular multiplication of scalars, and it means dot product for vectors.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A useful idea for converting an equation or a system of them into a matrix or vector equation is to recognize that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Vector-vector multiplication is same as the dot product of two vectors.&lt;/li&gt;
  &lt;li&gt;Dot product is simply elementwise multiplication followed by summation of the products.&lt;/li&gt;
  &lt;li&gt;Vector-matrix multiplication directly reduces to the dot product between the row or column vectors of a matrix and a vector. This makes vector-matrix multiplication, which is a subset of matrix multiplication, one example of tensor contraction. (We will revisit this later).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, when you see a pair of scalars getting multiplied and then the products from all such pairs are added (the formal name for this is linear combination), you should immediately suspect that such an equation may be easily substituted with a “tensorized” version.&lt;/p&gt;

&lt;table&gt;
&lt;td&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;b&gt;
What is a tensor?
&lt;/b&gt;
&lt;/summary&gt;
&lt;p&gt;

You probably already think of a vector as an array with one dimension (or axis). This makes it a first-order tensor, and a matrix is a second-order tensor as it has two axes. Similar objects with more than two axes are higher order tensors.
&lt;br /&gt;&lt;br /&gt;
In summary, a tensor is the generalization of vectors, matrices and higher order tensors. That is, a multidimensional array.
&lt;br /&gt;&lt;br /&gt;
But do note that in math, there is a lot more to tensor than just being a multidimensional array, just as there is much more to matrix than just being a 2D array. But this article is not concerned with that.

&lt;/p&gt;
&lt;/details&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;p&gt;The equations we’ve seen above are under the premise that we will be handling only one datapoint at a time. But we need to be able to handle more than one datapoint simultanously (we also need this when we start looking into neural networks because operations on matrices are easily parallelized). For this reason, we will do one more important thing to the equations we’ve seen above, which is to take them to matrix form.&lt;/p&gt;

&lt;p&gt;Improvement in parallelized computing is a huge reason deep learning returned to the spotlight in the last decade. Parallelization is also the reason GPUs have become a champion for machine learning, because they have thousands of cores unlike CPUs which typically have cores that number in the single digits.&lt;/p&gt;

&lt;p&gt;Going back to our toy dataset, if we wanted to compute preactivations for the first three datapoints at once, we get these three equations (and please always keep in mind that &lt;script type=&quot;math/tex&quot;&gt;w_0=b&lt;/script&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w_1\ \cdot6.5233+w_2\ \cdot1.5484+w_0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w_1\ \cdot9.2112+w_2\ \cdot12.7141+w_0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w_1\ \cdot1.7315+w_2\ \cdot45.6200+w_0&lt;/script&gt;

&lt;p&gt;Clearly, we need a new subscript to keep track of multiple datapoints, because it’s misleading to keep equating every datapoint to just &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. So, we do something like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_j=w_1\ \cdot x_{1,j}+w_2\ \cdot x_{2,j}+\ldots+w_n\ \cdot x_{n,j}+w_0=\sum_{i=1}^{n}{w_i\ \cdot x_{i,j}+b}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where the subscript $ j $ keeps track of datapoints. Or you can think of it as, $ i $ tracks the columns and $ j $ tracks rows in our toy dataset. Note that $ w_0 $ is same as $ b $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;So now we can write them as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_1=w_1\ \cdot6.5233+w_2\ \cdot1.5484+b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_2=w_1\ \cdot9.2112+w_2\ \cdot12.7141+b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_3=w_1\ \cdot1.7315+w_2\ \cdot45.6200+b&lt;/script&gt;

&lt;p&gt;Note that the numerical subscript on &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; above is not counterpart to that on &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;. The former tracks datapoints (rows in our toy dataset), and the latter tracks features (columns in our toy dataset). It’s all much clearer with algebra.&lt;/p&gt;

&lt;p&gt;You can already notice the system of equations. And if it had been a batch of 100 datapoints, or even the entire dataset, it starts becoming unwieldy to carry around thousands of equations. Therefore we vectorize!&lt;/p&gt;

&lt;p&gt;We summarize the preactivations for all the datapoints in our batch:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{z}=all\ z_j\ in\ the\ batch=[\begin{matrix}z_1&amp;z_2&amp;\cdots&amp;z_m\\\end{matrix}] %]]&gt;&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ m $ is the number of datapoints in our batch.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;What’s the batch all about? In deep learning, it’s very common to deal with very large datasets that may be too big or inefficient to load into memory all at once, so we sample out a portion of our dataset, we call it a batch, and we use it to train our model. That’s one iteration. We repeat the sampling for the second iteration, and continue for as many iterations as we choose to.&lt;/p&gt;

&lt;p&gt;Now we have all the ingredients to convert to matrix format. Our system of equation, will go from this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_1=w_1\ \cdot x_{1,1}+w_2\ \cdot x_{2,1}+\ldots+w_n\ \cdot x_{n,1}+w_0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_2=w_1\ \cdot x_{1,2}+w_2\ \cdot x_{2,2}+\ldots+w_n\ \cdot x_{n,2}+w_0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vdots&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_m=w_1\ \cdot x_{1,m}+w_2\ \cdot x_{2,m}+\ldots+w_n\ \cdot x_{n,m}+w_0&lt;/script&gt;

&lt;p&gt;To this matrix equation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{z}\ =\ \vec{w}\mathbf{X}\ + \vec{b}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Note that the lack of any symbols between $ \vec{w} $ and $ \mathbf{X} $ signifies matrix-vector multiplication, i.e. matrix multiplication between vector and matrix.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;I encourage you to rework the matrix equation back into the flat form if you’re unclear on how the two are the same. I promise, it will be a great refresher of math you probably saw in high school or first year of university.&lt;/p&gt;

&lt;p&gt;The variable &lt;script type=&quot;math/tex&quot;&gt;\vec{z}&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector, and if only one datapoint, will be a vector of only one entry (which is equivalent to a scalar).&lt;/p&gt;

&lt;p&gt;The parameter &lt;script type=&quot;math/tex&quot;&gt;\vec{w}&lt;/script&gt; is always going to be a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; vector, regardless of the number of datapoints. Its size depends on the number of features &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{w}=\left[\begin{matrix}w_1\\w_2\\\vdots\\w_n\\\end{matrix}\right]^T&lt;/script&gt;

&lt;p&gt;The variable &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector. Fundamentally, however, the bias is a scalar (or a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; vector) regardless of the number datapoints in the batch.&lt;/p&gt;

&lt;p&gt;There is only one bias for a neuron, and it’s simply the weight for the bias node, just like each of the other weights. It only gets stretched into a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector to match the shape of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, so that the matrix equation is valid. The stretching involves repeating the elements to fill up the stretched-out vector. When coding in Python and using the NumPy library for your computations, it’s good to know that this stretching (also called &lt;a href=&quot;https://docs.scipy.org/doc/numpy/user/theory.broadcasting.html#array-broadcasting-in-numpy&quot; target=&quot;_blank&quot;&gt;broadcasting&lt;/a&gt;)) is already baked into the library.&lt;/p&gt;

&lt;p&gt;Therefore the full answer for the shape of &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is that it is fundamentally a scalar (or a $ 1 $-by-$ 1 $ vector) that gets broadcasted into a vector of the right shape during the computation involved in the matrix equation for computing the preactivation. (If this still doesn’t make sense here, return to it later after you finish &lt;a href=&quot;/implement-an-artificial-neuron-from-scratch.html&quot; target=&quot;_blank&quot;&gt;part 4&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;We must keep in mind that &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is a parameter of the estimator, and it would be very counterproductive to define it in a way that binds it to the number of examples (datapoints) in a batch. This is why its fundamental form is a scalar.&lt;/p&gt;

&lt;p&gt;Here are some problems we would have if we defined &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; to be fundamentally a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector:&lt;/p&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        The neuron becomes restricted to a fixed batch size. That is, the batch size we use to train the neuron becomes a fixture of the neuron, to the point that we can’t use the neuron to carry out predictions or estimations for a different batch size.
&lt;br /&gt;&lt;br /&gt;
Each example in the batch will have a different corresponding value for $ b $. This is not even the case for $ w $, and it is just simply improper for the parameters to change from datapoint to datapoint. If that happened, then it means the model is not identical for all datapoints. Absolutely appalling.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; is broadcasted into the &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector &lt;script type=&quot;math/tex&quot;&gt;\vec{b}&lt;/script&gt;, it is simply the scalar value &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; repeating &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; times. It looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{b}=\left[\begin{matrix}b&amp;b&amp;\cdots&amp;b\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;The intuition is that you are applying the same bias to all the datapoint in any given batch, the same way you are applying the same group of weights to all the datapoint.&lt;/p&gt;

&lt;p&gt;Because the bias is fundamentally a scalar, it is normal to write the equation as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{z}\ =\vec{w}\mathbf{X}\ +b&lt;/script&gt;

&lt;p&gt;The variables &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt; will depend on the shape of the input data that gets fed to the neuron. It could be a vector or matrix (and in neural networks they could even be higher order tensors). When multiple datapoints, it’s an &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; matrix, and when a single datapoint it’s an &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; vector. It looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X}=\left[\begin{matrix}x_{1,1}&amp;x_{1,2}&amp;\cdots&amp;x_{1,m}\\x_{2,1}&amp;x_{2,2}&amp;\cdots&amp;x_{2,m}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_{n,1}&amp;x_{n,2}&amp;\cdots&amp;x_{n,m}\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Keep in mind that these statements about the shapes of these tensors are all for a single artificial neuron, as there are some changes when moving unto neural networks (a network of neurons).&lt;/p&gt;

&lt;p&gt;Let’s illustrate with our toy dataset how the preactivation equation works in matrix format. Let’s say we decide that our batch size will be 3, which means we will feed our neuron 3 datapoints (3 rows of our toy dataset), then our &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; will look like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X}=\left[\begin{matrix}6.5233&amp;9.2112&amp;1.7315\\1.5484&amp;12.7141&amp;45.6200\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;And the corresponding &lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt; is this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{y}=\ \left[\begin{matrix}0&amp;1&amp;0\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Let’s say we randomly initialize our weight vector to this (which is actually what is done in practice, but more like “controlled” randomization):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{w}=\left[\begin{matrix}w_1\\w_2\\\end{matrix}\right]^T=\left[\begin{matrix}0.5&amp;-0.3\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;And we set our bias to zero. Note that it will be a scalar, but broadcasted during computation to match whatever shape &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{z}&lt;/script&gt; has:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b=0&lt;/script&gt;

&lt;p&gt;Then we can compute our preactivation for this batch of 3 datapoints:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{z} =\ \left[\begin{matrix}0.5&amp;-0.3\\\end{matrix}\right]\left[\begin{matrix}6.5233&amp;9.2112&amp;1.7315\\1.5484&amp;12.7141&amp;45.6200\\\end{matrix}\right]\ +\left[\begin{matrix}0&amp;0&amp;0\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{z} =\left[\begin{matrix}2.79713&amp;0.79137&amp;-12.8202\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Let’s assume that the kind of artificial neuron we have is the original perceptron (that is, our activation function is the Heaviside function). Recall that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
a_j =
\begin{cases}
 1 &amp;\text{if } z_j &gt; 0 \\
0 &amp;\text{if } z_j \leq 0
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now we pass &lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{z}&lt;/script&gt; through a Heaviside function to obtain our activation value:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{a}=\left[\begin{matrix}1&amp;1&amp;0\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;Remember we already have the ground truth (&lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt;), so we can actually check and see how our (untrained) neuron did.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{y}=\ \left[\begin{matrix}0&amp;1&amp;0\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;And it did okay. It got the first datapoint wrong (it predicted high energy instead of the correct label of low energy) but got the other two right. That’s 66.7% accuracy. We likely won’t be this lucky if we use more datapoints.&lt;/p&gt;

&lt;p&gt;We can easily notice that &lt;script type=&quot;math/tex&quot;&gt;\vec{a}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\vec{z}&lt;/script&gt; will always have the same shape, which is a &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;-by-&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; vector; and if only one datapoint, will be a vector of only one entry (which is equivalent to a scalar).&lt;/p&gt;

&lt;p&gt;To improve the performance of the artificial neuron, we need to train it. That simply means that we need to find the right values for the parameters $ \vec{w} $ and $ b $ such that when we feed our neuron any datapoint from the dataset, it will estimate the correct energy level.&lt;/p&gt;

&lt;p&gt;This is the general idea of how the perceptron, or any other kind of artificial neuron, works. That is, we should be able to compute a set of parameters (&lt;script type=&quot;math/tex&quot;&gt;w_0,\ w_1,\ w_2,\ \ldots,\ w_n&lt;/script&gt;) such that the perceptron is able to produce the correct output when given an input.&lt;/p&gt;

&lt;p&gt;For instance, when fed the images of cats and dogs, a unit (an artificial neuron) with good parameters will correctly classify them. The pixels of the image will be the input, &lt;script type=&quot;math/tex&quot;&gt;x_1,x_2,\ \ldots,\ x_n&lt;/script&gt;, and the unit will do its math and output 0 or 1 (representing the two possible labels). Simple!&lt;/p&gt;

&lt;p&gt;This is the whole point of a neural network (a.k.a. network of artificial neurons). And that process of finding a good collection of parameters for a neuron (or a network of neurons as we will see later) is what we call “learning” or “training”, which is the same thing mathematicians call mathematical optimization.&lt;/p&gt;

&lt;p&gt;Unfortunately, the original perceptron did not fair very well in practice and failed to deliver on the high hopes heaped on it. I can assure you that it will not do too well with image classification of, say, cats and dogs. We need something more complex with some more nonlinearity.&lt;/p&gt;

&lt;p&gt;Note that linearity is not the biggest reason Heaviside functions went out of favour. In fact, a Heaviside function is not purely linear, but instead piecewise linear. It’s also common to see lack of differentiability at zero blamed for the disfavour, but again this is cannot be the critical reason, as there are cheap tricks around this too (e.g. the same type of schemes used to get around the undifferentiability of the rectified linear function at zero, which by the way is currently the most widely used activation function in deep learning).&lt;/p&gt;

&lt;p&gt;The main problem is that the Heaviside function jumps too rapidly, in fact instantaneously, between the two extremes of its range. That is, when traversing the domain of the Heaviside function, starting from positive to negative infinity, we will keep outputting one (the highest value in its range), until suddenly at the input of zero, its output snaps to 0 (the minimum value in its range) and then continues outputting that for the rest of negative infinity. This causes a lot of instability. When doing mathematical optimization, we typically prefer small changes to also produce small changes.&lt;/p&gt;

&lt;h2 id=&quot;activation-functions&quot;&gt;&lt;strong&gt;Activation functions&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;It is possible to use other kinds of functions as an activation function (a.k.a. transfer function), and this is indeed what researchers did when the original perceptron failed to deliver. One such replacement was the sigmoid function, which resembles a smoothened Heaviside function.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/heaviside_logistic.png&quot; alt=&quot;Plots of the Heaviside and logistic activation functions.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;Plots of the Heaviside and logistic activation functions.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Note that the term “sigmoid function” refers to a family of s-shaped functions, of which the very popular logistic function is one of them. As such, it is common to see logistic and sigmoid used interchangeably, even though they are strictly not synonyms.&lt;/p&gt;

&lt;p&gt;The logistic function performs better than the Heaviside function. In fact, machine learning using an artificial neuron that uses the logistic activation function is one and the same as logistic regression. Ha! You’ve probably run into that one before. Don’t feel left out if you haven’t though, because you’re just about to.&lt;/p&gt;

&lt;p&gt;This is the equation for logistic regression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\hat{y}}=\frac{1}{1+e^{-\vec{z}}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{z}\ =\ \vec{w}\mathbf{X}\ +\ b&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \boldsymbol{\hat{y}} $ is the prediction or estimation (just another name for activation). It is a 1-by-$ m $ vector. It's not the unit vector for $ \vec{y} $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;And this is the equation for an artificial neuron with a logistic (sigmoid) activation function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{a}=\frac{1}{1+e^{-\vec{z}}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{z}\ =\ \vec{w}\mathbf{X} + b&lt;/script&gt;

&lt;p&gt;As you can see, they are one and the same!&lt;/p&gt;

&lt;p&gt;Also note that the perceptron, along with every other kind of artificial neuron, is an estimator just like other machine learning models (linear regression, etc.).&lt;/p&gt;

&lt;p&gt;Besides the sigmoid and Heaviside functions, there are a plethora of other functions that have found great usefulness as activation functions. &lt;strong&gt;You can find a list of many other activation functions in &lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Activation_function&amp;amp;oldid=939349877#Comparison_of_activation_functions&quot; target=&quot;_blank&quot;&gt;this Wikipedia article&lt;/a&gt;&lt;/strong&gt;. You should take note of the rectified linear function; any neuron using it is known as a rectified linear unit (ReLU). It’s the most popular activation function in deep learning as of 2020, and will likely remain so in the foreseeable future.&lt;/p&gt;

&lt;p&gt;One more important mention is that the process of going from input data (&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;) all the way to activation (essentially, the execution of an activation function) is called &lt;strong&gt;forward pass&lt;/strong&gt; (or forward propagation in the context of neural networks), and this is the process we demonstrated above using the toy dataset. This distinguishes from the sequel process, known as &lt;strong&gt;backward pass&lt;/strong&gt;, where we use the error between the activation (&lt;script type=&quot;math/tex&quot;&gt;\vec{a}&lt;/script&gt;) and the ground truth (&lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt;) to tune our parameters in such a way that the error decreases.&lt;/p&gt;

&lt;p&gt;To tie things back to our toy dataset. If we used a logistic activation function instead of a Heaviside function, and trained our neuron for 2000 iterations, we obtain some values for the parameters that gives us the correct result 91% of the time. (We will later go over exactly what happens during “training”).&lt;/p&gt;

&lt;p&gt;The parameters after training are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{w}=\left[\begin{matrix}0.33456&amp;0.0206573\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b=-2.09148&lt;/script&gt;

&lt;p&gt;So, the optimized (trained) equation for our artificial neuron is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{z}\ =\ \left[\begin{matrix}0.33456&amp;0.0206573\\\end{matrix}\right]\mathbf{X}\ +\left[\begin{matrix}-2.09148&amp;-2.09148&amp;-2.09148\\\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vec{a}=\frac{1}{1+e^{-\vec{z}}}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ \mathbf{X} $ is an $ n $-by-$ m $ matrix that contains a batch of our dataset, and $ m $ is the number of datapoints in our batch, while $ n $ is the number of features in our dataset.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The above is the logistic artificial neuron that has learned the relationship hidden in our toy dataset, and you can randomly pick some datapoints in our dataset and verify the equation yourself. Roughly 9 out of 10 times, it should produce an activation that matches the ground truth (&lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;Note that the values for the parameters are not unique. A different set of values can still give us a comparable performance. We only discovered one of many possible sets of values that can give us good performance.&lt;/p&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;&lt;strong&gt;Loss function&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&quot;/some-musings-about-ai.html#estimators&quot; target=&quot;_blank&quot;&gt;section on estimators&lt;/a&gt; from part 1 of this blog series, I mentioned that it is imperative to expect an estimator (which is what an artificial neuron is) to have some level of error in its prediction, and our objective will be to minimize this error.&lt;/p&gt;

&lt;p&gt;We described this error as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon =y-\hat{y}=f_{actual} \left( x \right) -f_{estim} \left( x \right)&lt;/script&gt;

&lt;p&gt;The above is actually one of the many ways to describe the error, and not rigorous enough. For example, it is missing an absolute value operator, else the sign of the error will change just based on how the operands of the subtraction are arranged:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y-\hat{y}\neq\hat{y}-y&lt;/script&gt;

&lt;p&gt;For instance, we know the difference between the &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_number&quot; target=&quot;_blank&quot;&gt;natural numbers&lt;/a&gt;) 5 and 3 is 2, but depending on how you rearrange the subtraction between them, we could end up with -2 instead, and we don’t want that to be happening, so we apply an absolute value operation and restate the error as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon=|y-\hat{y}|&lt;/script&gt;

&lt;p&gt;Now if we have a dataset made of more than one datapoint, we will have many errors, one for each datapoint. We need a way to aggregate all those individual errors into one big error value that we call the loss (or cost).&lt;/p&gt;

&lt;p&gt;We achieve this by simply averaging all those errors to produce a quantity we call the mean absolute error:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Mean\ absolute\ error=Cost=\frac{1}{m} \cdot \sum _{j=0}^{m} \vert y_{j}-\hat{y}_{j} \vert =\frac{1}{m} \cdot  \sum _{j=0}^{m} \varepsilon _{j}&lt;/script&gt;

&lt;div&gt;
    &lt;p style=&quot;margin-left:10%; margin-right:10%;&quot;&gt;
        Where $ m $ is the number of datapoints in the batch of data. Note that $ \hat{y}_{j} $ is same as activation $ a_j $, and it is denoted here as such to show that it serves as an estimate for the ground truth $ y_j $.
    &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The above equation happens to be just one of the many types of loss functions (a.k.a. cost function) in broad use today. They all have one thing in common: They produce &lt;strong&gt;a single scalar value (the loss or cost)&lt;/strong&gt; that captures how well our network has learned the relationship between the features and the target for a given batch of a dataset.&lt;/p&gt;

&lt;table&gt;
&lt;td&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;b&gt;Cost function vs loss function vs objective function&lt;/b&gt;
&lt;/summary&gt;
&lt;p&gt;
Some reserve the term loss function for when dealing with one datapoint and use cost function for the version that handles a batch of multiple datapoints.
&lt;br /&gt;&lt;br /&gt;
An objective function is simply the function that gets optimized in order to solve an optimization problem. In deep learning the loss or cost function plays that role, therefore making objective function another name for loss or cost function.&lt;/p&gt;
&lt;/details&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;p&gt;We will introduce two other loss functions that are very widely used.&lt;/p&gt;

&lt;p&gt;Mean squared error loss function, which is typically used for regression tasks:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Mean\ squared\ error:\ J=\frac{1}{m}\cdot\sum_{j}^{m}\left(y_j-a_j\right)^2&lt;/script&gt;

&lt;p&gt;You might have seen the above equation before if you’ve learned about linear regression.&lt;/p&gt;

&lt;p&gt;Logistic loss function (also known as cross entropy loss or negative log-likelihoods), which is typically used for classification tasks, is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cross\ entropy\ loss:\ \ J = -\frac{1}{m}\cdot\sum_{j}^{m}{y_j\cdot\log{(a_j)}+(1-y_j)\cdot\log{({1-a}_j)}}=\frac{1}{m}\cdot\sum_{j=0}^{m}\varepsilon_j&lt;/script&gt;

&lt;p&gt;Note that the logarithm in the cross entropy loss is with base &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; (Euler’s number). In other words, it is a natural logarithm, which is sometimes abbreviated as &lt;script type=&quot;math/tex&quot;&gt;\ln&lt;/script&gt; instead of &lt;script type=&quot;math/tex&quot;&gt;\log&lt;/script&gt;. Also note that we are implicitly assuming that our ground truth is binary (i.e. only two classes and therefore binary classification).&lt;/p&gt;

&lt;p&gt;Notice that all these loss functions have one thing in common, they are all functions of activation, which also makes them functions of the parameters:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cost:\ \ J=f\left(\vec{a}\right)=f(\vec{w},b)\&lt;/script&gt;

&lt;p&gt;For instance, cross entropy loss function for a single datapoint can be recharacterized as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Cross\ entropy\ loss=\ -\left(y\cdot\log{a})+(1-y)\cdot\log(1-a)\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=-\left(data+\left(1-\frac{1}{1+e^{-z}}\right)\cdot\log{\left(1-\frac{1}{1+e^{-z}}\right)}\right)\&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=-\left(data\cdot\log{\left(\frac{1}{1+e^{-z}}\right)}+\left(1-data\right)\cdot\log{\left(1-\frac{1}{1+e^{-z}}\right)}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=-\left(data\cdot\log{\left(\frac{1}{1+e^{\sum_{i=0}^{n}{w_i\ \cdot x_i}}}\right)}+\left(1-data\right)\cdot\log{\left(1-\frac{1}{1+e^{-\sum_{i=0}^{n}{w_i\ \cdot x_i}}}\right)}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=-\left(data+\left(1-\frac{1}{1+e^{\sum_{i=0}^{n}{w_i\ \cdot\ data}}}\right)\cdot\log{\left(1-\frac{1}{1+e^{-\sum_{i=0}^{n}{w_i\ \cdot\ data}}}\right)}\right)\&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=-\left(data\cdot\log{\left(\frac{1}{1+e^{\sum_{i=0}^{n}{w_i\ \cdot d a t a}}}\right)}+\left(1-data\right)\cdot\log{\left(1-\frac{1}{1+e^{-\sum_{i=0}^{n}{w_i\ \cdot data}}}\right)}\right)\&lt;/script&gt;

&lt;p&gt;In other words, the loss function can be described purely as a function of the parameters (&lt;script type=&quot;math/tex&quot;&gt;\vec{w}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;) and the data (&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\vec{y}&lt;/script&gt;). And since data is known, the only unknowns on the right-hand side of the equation are the parameters.&lt;/p&gt;</content><author><name>Prince Okoli</name></author><summary type="html"></summary></entry><entry><title type="html">Catching AI with its pants down: Some Musings About AI</title><link href="http://localhost:4000/some-musings-about-ai.html" rel="alternate" type="text/html" title="Catching AI with its pants down: Some Musings About AI" /><published>2020-03-13T00:00:00-06:00</published><updated>2020-03-13T00:00:00-06:00</updated><id>http://localhost:4000/some-musings-about-ai</id><content type="html" xml:base="http://localhost:4000/some-musings-about-ai.html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            processEscapes: true
          }
        });
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_CHTML&quot;&gt;
&lt;/script&gt;

&lt;table&gt;
&lt;td&gt;
&lt;i&gt;We will strip the mighty, massively hyped, highly dignified AI of its cloths, and bring its innermost details down to earth!&lt;/i&gt;
&lt;/td&gt;
&lt;/table&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#objective&quot; id=&quot;markdown-toc-objective&quot;&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#motivation&quot; id=&quot;markdown-toc-motivation&quot;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#artificial-general-intelligence-the-holy-grail-of-ai&quot; id=&quot;markdown-toc-artificial-general-intelligence-the-holy-grail-of-ai&quot;&gt;&lt;strong&gt;Artificial General Intelligence: The Holy Grail of AI&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#artificial-narrow-intelligence&quot; id=&quot;markdown-toc-artificial-narrow-intelligence&quot;&gt;&lt;strong&gt;Artificial Narrow Intelligence&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#machine-learning&quot; id=&quot;markdown-toc-machine-learning&quot;&gt;&lt;strong&gt;Machine learning&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#supervised-vs-unsupervised-learning&quot; id=&quot;markdown-toc-supervised-vs-unsupervised-learning&quot;&gt;&lt;strong&gt;Supervised vs. Unsupervised Learning&lt;/strong&gt;&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#supervised-learning-regression-vs-classification&quot; id=&quot;markdown-toc-supervised-learning-regression-vs-classification&quot;&gt;&lt;strong&gt;Supervised learning: Regression vs Classification&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforcement-learning&quot; id=&quot;markdown-toc-reinforcement-learning&quot;&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#deep-learning&quot; id=&quot;markdown-toc-deep-learning&quot;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#estimators&quot; id=&quot;markdown-toc-estimators&quot;&gt;&lt;strong&gt;Estimators&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;objective&quot;&gt;&lt;strong&gt;Objective&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The goal of this writeup is to present modern artificial intelligence (AI), which is largely powered by deep neural networks, in a highly accessible form. I will walk you through building a deep neural network from scratch without reliance on any machine learning libraries and we will use our network to tackle real public research datasets.&lt;/p&gt;

&lt;p&gt;To keep this very accessible, all the mathematics will be simplified to a level that anyone with a high-school or first-year-university level of math knowledge and that can code (especially if Python) should be able to follow. Together we will strip the mighty, massively hyped, highly dignified AI of its cloths, and bring its innermost details down to earth. When I say AI here, I’m being a little silly with buzzspeak and actually mean deep neural networks.&lt;/p&gt;

&lt;p&gt;However,
all the code presented in this blog series can be found at &lt;a href=&quot;https://github.com/princyok/deep_learning_without_ml_libraries&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;this GitHub repo&lt;/strong&gt;&lt;/a&gt;, and includes code for artificial neuron and deep neural networks from scratch. Even the codes for the latter articles are already available there.&lt;/p&gt;

&lt;p&gt;This writeup aims to be very detailed, simple and granular, such that by the end, you hopefully should have enough knowledge to investigate and code more advanced architectures from scratch if you chose to do so. You should expect a lot of math, but don’t let that scare you away, as I’ll tried my best to explain things as simply as possible.&lt;/p&gt;

&lt;p&gt;The original plan was to explain everything in one giant article, but that quickly proved unwieldy. So, I decided to break things up into multiple articles. This first article is basically some casual ramblings about AI with a brief overview of machine learning, and subsequent articles focus on introducing an artificial neuron, the derivation of the equations needed to build one from scratch and the code implementation of those equations, and then repeat all that for a network of artificial neurons (a.k.a. neural networks).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parts&lt;/th&gt;
      &lt;th&gt;Catching AI with its pants down&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 1&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/some-musings-about-ai.html&quot;&gt;&lt;strong&gt;Some Musings About AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 2&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/understand-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Understand an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 3&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/optimize-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Optimize an Artificial Neuron from Scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 4&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;/implement-an-artificial-neuron-from-scratch.html&quot;&gt;&lt;strong&gt;Implement an artificial neuron from scratch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 5&lt;/td&gt;
      &lt;td&gt;Understand a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 6&lt;/td&gt;
      &lt;td&gt;Optimize a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pant 7&lt;/td&gt;
      &lt;td&gt;Implement a neural network from scratch (coming soon)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;motivation&quot;&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;My feeling is if you want to understand a really complicated device like a brain, you should build one. I mean, you can look at cars, and you could think you could understand cars. When you try to build a car, you suddenly discover then there’s this stuff that has to go under the hood, otherwise it doesn’t work.&lt;/p&gt;

&lt;p&gt;The entirety of the above paragraph is one of my favourite quotes by Geoffrey Hinton, one of the three Godfathers of Deep Learning. I don’t think we need any further motivation for why we should peek under the hood to see precisely what’s really going on in a deep learning AI system.&lt;/p&gt;

&lt;p&gt;Tearing apart whatever is under the hood has been my canon for my machine learning journey, so it was natural that I would build a deep neural network from scratch especially after I couldn’t find any such complete implementation online (as of early 2019). Some of my colleagues thought it would be good if I put together some explanation of what I did, and so the idea for this writeup was born.&lt;/p&gt;

&lt;h2 id=&quot;artificial-general-intelligence-the-holy-grail-of-ai&quot;&gt;&lt;strong&gt;Artificial General Intelligence: The Holy Grail of AI&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Artificial intelligence (AI) is the intelligence, or the impression thereof, exhibited by things made by we humans. The kind of intelligence we have is natural intelligence. A lot of things can fall under the umbrella of AI because the definition is vague. Everything from the computer player of Chess Titans on Windows 7 to Tesla’s autopilot is called AI.&lt;/p&gt;

&lt;p&gt;Artificial general intelligence (AGI) is the machine intelligence that can handle anything a human can. You can think of the T-800 from &lt;em&gt;The Terminator&lt;/em&gt; or Sonny from &lt;em&gt;I, Robot&lt;/em&gt; (although in my opinion, the movie’s view of AI, at least with regards to Sonny, aligns more with symbolic, rule-based AI instead of machine learning). Such AI system is also referred to as strong AI.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/t800_terminator.png&quot; alt=&quot;The T-800 Terminator, a classic imagination of a strong AI that can learn through verbal interactions and solve problems on the fly.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;The T-800 Terminator, a classic imagination of a strong AI that can learn through verbal interactions and solve problems on the fly.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;AGI would be able to solve problems that were not explicitly specified in its design phase.
There is no AGI system in existence today, nor is there any research group that is known to be anywhere close to deploying one. In fact, there is not even a semblance of consensus on when AGI could become reality.&lt;/p&gt;

&lt;p&gt;Tech author Martin Ford, for his 2018 book &lt;em&gt;Architects of Intelligence&lt;/em&gt;, surveyed 23 leading AI figures about when there would be a 50 percent chance of AGI being built. Those surveyed included DeepMind CEO Demis Hassabis, Head of Google AI Jeff Dean, and Geoffrey Hinton (one of the three Godfathers of Deep Learning).&lt;/p&gt;

&lt;p&gt;Of the 23 surveyed, 16 answered anonymously, and 2 answered with their names. The most immediate estimate of 2029 came from Google director of engineering Ray Kurzweil and the most distant estimate of 2200 came from Rod Brooks (the former director of MIT’s AI lab and co-founder of iRobot). The average estimate was 2099.&lt;/p&gt;

&lt;p&gt;There are many other surveys out there that give results in the 2030s and 2040s. I feel this is because people have a tendency to want the technologies that they are hopeful about to become reality in their lifetimes, so they tend to guess 20 to 30 years from the present, because that’s long enough time for a lot of progress to be made in any field and short enough to fit within their lifetime.&lt;/p&gt;

&lt;p&gt;For instance, I too get that gut feeling that space propulsions that can reach low-end relativistic speeds should be just 20 to 40 years away; how else will the Breakthrough Starshot (founded by  Zuckerberg, Milner and the late Hawking) get a spacecraft to Proxima Centauri b. Same for fault-tolerant quantum computers, fusion power with gain factor far greater than 1, etc. They are all just 20 to 40 years away, because these are all things I really want to see happen.&lt;/p&gt;

&lt;p&gt;Also, it seems that &lt;a href=&quot;https://blog.aimultiple.com/artificial-general-intelligence-singularity-timing/&quot; target=&quot;_blank&quot;&gt;AI entrepreneurs tend to be much more optimistic about how close we are to AGI than AI researchers&lt;/a&gt; are. Someone should do a bigger survey for that, ha!&lt;/p&gt;

&lt;h2 id=&quot;artificial-narrow-intelligence&quot;&gt;&lt;strong&gt;Artificial Narrow Intelligence&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The type of AI we interact with today and hear of nonstop in the media is artificial narrow intelligence (ANI), also known as weak AI. It differs from AGI in that the AI is designed to deal with a specific task or a specific group of closely related tasks. Some popular examples are AlphaGo, Google Assistant, Alexa, etc.&lt;/p&gt;

&lt;p&gt;A lot of the hype that has sprung up around ANI in the last decade was driven by the progress made with applying deep neural networks (a.k.a. deep learning) to supervised learning tasks (we will talk more about these below) and more recently to reinforcement learning tasks.&lt;/p&gt;

&lt;p&gt;A supervised learning task is one were the mathematical model (what we would call the AI if we’re still doing buzzspeak) is trained to associate inputs with their correct outputs, so that it can later produce a correct output when fed an input it never saw during training. An example is when &lt;a href=&quot;https://lens.google.com/&quot; target=&quot;blank&quot;&gt;Google Lens&lt;/a&gt; recognizes the kind of shoe you are pointing the camera at, or when IBM Watson transcribes your vocal speech to text. Google Lens can recognize objects in images because the neural network powering it has been trained with images where the objects in them have been correctly labelled, so that when it later sees a new image it has never seen before, it can still recognize patterns that it already learned during training.&lt;/p&gt;

&lt;p&gt;In reinforcement learning, you have an agent that tries to maximize future cumulative reward by exploring and exploiting the environment. That’s what DeepMind’s &lt;a href=&quot;https://www.youtube.com/watch?v=WXuK6gekU1Y&quot; target=&quot;_blank&quot;&gt;AlphaGo&lt;/a&gt; is in a nutshell. It takes in the current board configuration as input data and spits out the next move to play that will maximize the chances of winning the match.&lt;/p&gt;

&lt;p&gt;The important point is that deep neural networks have been a key transformative force in the development of powerful ANI solutions in recent times.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning&quot;&gt;&lt;strong&gt;Machine learning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The rise of the deep learning hype has been a huge boon for its parent field of machine learning. Machine learning is simply the study of building computers systems that can “learn” from examples (i.e. data). The reason for the quotes around “learn” is that the term is just a machine learning lingo for &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_optimization&quot; target=&quot;_blank&quot;&gt;mathematical optimization&lt;/a&gt; (and we will talk more about this later). We will also use the term “training” a lot, and it also refers to the same mathematical optimization.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/training_vs_test_cat_dog_illustration.png&quot; alt=&quot;In machine learning, the model learns the associations presented in the training set; that is, images with certain kinds of patterns, which we humans effortlessly recognize as characteristics of a cat or dog, map to a certain label (cat or dog). It uses the knowledge learned to correctly label the images in the test set, which are images it never saw during training. This is specifically supervised learning, a category of machine learning where the computer program is provided with correctly labelled examples to learn from.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;In machine learning, the model learns the associations presented in the training set; that is, images with certain kinds of patterns, which we humans effortlessly recognize as characteristics of a cat or dog, map to a certain label (cat or dog). It uses the knowledge learned to correctly label the images in the test set, which are images it never saw during training. This is specifically supervised learning, a category of machine learning where the computer program is provided with correctly labelled examples to learn from.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In machine learning, you have a model that takes in data and spits out something relevant to that data. For the task of labelling images of cats and dogs (see image above), a model will receive images as input data and then it will output the correct labels for those images. This is a task that is trivial for humans, but was practically impossible for computer programs to consistently perform well at until convolutional neural networks came along. This is because it is extremely laborious to manually write programs to identify all the patterns needed to identify the primary object in the image, which leaves machine learning as a more feasible route.&lt;/p&gt;

&lt;figure class=&quot;image&quot; align=&quot;middle&quot;&gt;
  &lt;img src=&quot;/assets/images/artificial_neuron/image_to_numbers.png&quot; alt=&quot;A digital image is just a collection of pixels, and each pixel is simply a box shaded with one color. For a greyscale image like above, there is only one color with intensity ranging from 0 (for pure black) to 256 (for pure white). For machine learning, we simply covert the image to a collection of numbers, e.g. an array or matrix.&quot; center-image=&quot;middle&quot; /&gt;
  &lt;figcaption&gt;&lt;i&gt;A digital image is just a collection of pixels, and each pixel is simply a box shaded with one color. For a greyscale image like above, there is only one color with intensity ranging from 0 (for pure black) to 256 (for pure white). For machine learning, we simply covert the image to a collection of numbers, e.g. an array or matrix.&lt;/i&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;supervised-vs-unsupervised-learning&quot;&gt;&lt;strong&gt;Supervised vs. Unsupervised Learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The two main broad categories of machine learning are supervised learning and unsupervised learning. The main distinction between the two is that in the former the program is provided with a target variable (or labelled data in the in the context of classification) and in the latter, no variable is designated as the target variable.&lt;/p&gt;

&lt;p&gt;But don’t let the “supervision” in the name fool you, because, as of 2020, working on real-world unsupervised tasks requires more “supervision” (in the form of domain-specific tweaks) than supervised tasks (which can still benefit from domain-specific tweaks). But there is a general sense of expectation that unsupervised learning will start rivalling the success of supervised learning in terms of practical effectiveness (and also hype) within the next few years.&lt;/p&gt;

&lt;p&gt;In supervised learning, the dataset will have two part. One is the &lt;strong&gt;target&lt;/strong&gt; variable (&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;) that holds the values to be predicted, and the other is the rest of the data (&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;), which are also called the input variables, independent variables, predictors, or &lt;strong&gt;features&lt;/strong&gt;. The target variable &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is also called the output variable, response, or dependent variable, ground truth. It’s quite useful to be able to recognize all these alternative names.&lt;/p&gt;

&lt;p&gt;For instance, in an image classification task, the pixels of the image is the input variables and the label for the image is the target.&lt;/p&gt;

&lt;p&gt;A datapoint also goes by many names in the ML community. Names like “example”, “instance”, “record”, “observation”, etc., are all monikers for “datapoint”. I may use examples and records as alternatives to datapoint every now and then in this blog series, but I will mostly stick to using datapoint.&lt;/p&gt;

&lt;h4 id=&quot;supervised-learning-regression-vs-classification&quot;&gt;&lt;strong&gt;Supervised learning: Regression vs Classification&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The two broad categories of supervised learning are classification and regression. In classification, the target variable has discrete values, e.g. cat and dog labels. There can’t be a value between cat and dog. It’s either a cat or a dog. Other examples would be a variable that holds labels for whether an email is spam or not spam, or labels for hair color, etc.&lt;/p&gt;

&lt;p&gt;In regression, the target variable has continuous values, e.g. account balances. It could be -\$50 or \$20 or some number in between that. It could be floating point number like \$188.5555, or really large positive or negative number.&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning&quot;&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Another subset of machine learning that some consider a category of its own alongside supervised learning and unsupervised learning is reinforcement learning. It’s about building programs that take actions that affect an environment in such a way that the cumulative future reward is maximized; in other words, programs that love to win!&lt;/p&gt;

&lt;p&gt;You may run into other sources that consider it a hybrid of both supervised and unsupervised learning (i.e. semi-supervised learning). This is debatable because there is no label or correction involved in the training process, but there is a reward system that guides the learning process.&lt;/p&gt;

&lt;p&gt;Also be careful, because reinforcement learning is not a definitive name for the hybrids of the two. There are other subsets of machine learning that are truer hybrids of supervised and unsupervised learning but do not fall under reinforcement learning. For instance, generative adversarial neural networks (the family of machine learning models behind the &lt;a href=&quot;https://www.youtube.com/watch?v=cQ54GDm1eL0&quot; target=&quot;_blank&quot;&gt;deepfake technology&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;deep-learning&quot;&gt;&lt;strong&gt;Deep Learning&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Deep learning is simply machine learning that focuses on deep neural networks, which is a network of artificial neurons stacked into several layers. Deep neural networks can be used for supervised, unsupervised and reinforcement learning. As such, deep learning can intersect with all three major categories of machine learning.&lt;/p&gt;

&lt;p&gt;However, much of the advances we’ve seen with deep learning in the last two decades has been for supervised learning and reinforcement learning. But there is a lot of ongoing work to make deep learning work great for unsupervised learning as it has for the other two.&lt;/p&gt;

&lt;h2 id=&quot;estimators&quot;&gt;&lt;strong&gt;Estimators&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;When you see an apple, you are able to recognize that the fruit is an apple. When the accelerator (gas pedal) of a car is pressed down, the velocity of the car changes. When you see a ticktacktoe board where the game is ongoing, a decision on what is the best next move emerges.&lt;/p&gt;

&lt;p&gt;All of these have one thing in common: there is a process that takes an input and spits out an output. The visuals of an apple is the input and the recognition of the name is the output. Pressing down of the accelerator is an input and the rate of change of velocity is the output.
All of these processes can be thought of as functions. A function is the mapping of a set of inputs to a set of outputs in such a way that no two or more inputs will result in the same output. Almost any process can be thought of as a function. The hard part is fully characterizing the function that underlies a process.&lt;/p&gt;

&lt;p&gt;An estimator is a function that tries to estimate the behavior of another function whose details are not fully unknown.&lt;/p&gt;

&lt;p&gt;An estimator is the core component of a supervised machine learning system. It goes by many other names including being simply called the model, approximator, hypothesis function, learner, etc. But note that some of these other names, like model and learner, can also refer to more than just the estimator. For example, model can refer to an entire software system instead of just the mathematical model.&lt;/p&gt;

&lt;p&gt;Let’s say there is a function (&lt;script type=&quot;math/tex&quot;&gt;f_{actual}&lt;/script&gt;) that takes &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; as an input and spits out &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, then an estimator (&lt;script type=&quot;math/tex&quot;&gt;f_{estim}&lt;/script&gt;) will take in the same &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; as its input and will spit out &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; as an output. This  &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; will be expected to be approximately same as &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=f_{actual}\left(x\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}=f_{estim}\left(x\right)&lt;/script&gt;

&lt;p&gt;Because we expect that there may be a difference between &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{y}&lt;/script&gt; (preferrably a very small difference), we introduce an error term to capture that difference:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon=y-\hat{y}&lt;/script&gt;

&lt;p&gt;Therefore, we can then see that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\hat{y}+\varepsilon&lt;/script&gt;

&lt;p&gt;Or written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=f_{actual}\left(x\right)=f_{estim}\left(x\right)+\varepsilon&lt;/script&gt;

&lt;p&gt;We notice that if we can minimize &lt;script type=&quot;math/tex&quot;&gt;\varepsilon&lt;/script&gt; down to a really small value, then we can have an estimator that behaves like the real function that we are trying to estimate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y \approx \hat{y}=f_{estim}\left(x\right)&lt;/script&gt;

&lt;p&gt;We will revisit the error when we go over the loss function for an artificial neuron.&lt;/p&gt;

&lt;p&gt;If you’ve heard of naïve Bayes, logistic regression, linear regression, or k-nearest neighbours, then you’ve heard of other examples of machine learning estimators. But those are not the focus of this blog series (although logistic regression is kind of), nor do you need to know how those work to follow along in this series.&lt;/p&gt;</content><author><name>Prince Okoli</name></author><summary type="html"></summary></entry></feed>